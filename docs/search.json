[
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "Machine Learning Pipline for Regression",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.3.0\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.2.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\nlibraries for data reading\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(visdat)\nlibrary(ggpubr)\nlibrary(skimr)\nlibrary(broom)"
  },
  {
    "objectID": "hyperparameter-tuning.html#question-what-are-the-main-predictors-of-mean-streamflow-in-the-poudre-basin",
    "href": "hyperparameter-tuning.html#question-what-are-the-main-predictors-of-mean-streamflow-in-the-poudre-basin",
    "title": "Machine Learning Pipline for Regression",
    "section": "1. Question: What are the main predictors of mean streamflow in the Poudre basin?",
    "text": "1. Question: What are the main predictors of mean streamflow in the Poudre basin?"
  },
  {
    "objectID": "hyperparameter-tuning.html#data-have-already-been-read-in",
    "href": "hyperparameter-tuning.html#data-have-already-been-read-in",
    "title": "Machine Learning Pipline for Regression",
    "section": "2.Data have already been read in",
    "text": "2.Data have already been read in"
  },
  {
    "objectID": "hyperparameter-tuning.html#summarizing-the-data",
    "href": "hyperparameter-tuning.html#summarizing-the-data",
    "title": "Machine Learning Pipline for Regression",
    "section": "3. Summarizing the Data",
    "text": "3. Summarizing the Data\n\nBasic Attributes with Skim\n\nskim(camels)\n\n\nData summary\n\n\nName\ncamels\n\n\nNumber of rows\n671\n\n\nNumber of columns\n60\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n54\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngauge_id\n0\n1.00\n8\n8\n0\n671\n0\n\n\nhigh_prec_timing\n0\n1.00\n3\n3\n0\n4\n0\n\n\nlow_prec_timing\n0\n1.00\n3\n3\n0\n4\n0\n\n\ngeol_1st_class\n0\n1.00\n12\n31\n0\n12\n0\n\n\ngeol_2nd_class\n138\n0.79\n12\n31\n0\n13\n0\n\n\ndom_land_cover\n0\n1.00\n8\n34\n0\n12\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\n…1\n0\n1.00\n336.00\n193.85\n1.00\n168.50\n336.00\n503.50\n671.00\n▇▇▇▇▇\n\n\np_mean\n0\n1.00\n3.26\n1.41\n0.64\n2.37\n3.23\n3.78\n8.94\n▃▇▂▁▁\n\n\npet_mean\n0\n1.00\n2.79\n0.55\n1.90\n2.34\n2.69\n3.15\n4.74\n▇▇▅▂▁\n\n\np_seasonality\n0\n1.00\n-0.04\n0.53\n-1.44\n-0.26\n0.08\n0.22\n0.92\n▁▂▃▇▂\n\n\nfrac_snow\n0\n1.00\n0.18\n0.20\n0.00\n0.04\n0.10\n0.22\n0.91\n▇▂▁▁▁\n\n\naridity\n0\n1.00\n1.06\n0.62\n0.22\n0.70\n0.86\n1.27\n5.21\n▇▂▁▁▁\n\n\nhigh_prec_freq\n0\n1.00\n20.93\n4.55\n7.90\n18.50\n22.00\n24.23\n32.70\n▂▃▇▇▁\n\n\nhigh_prec_dur\n0\n1.00\n1.35\n0.19\n1.08\n1.21\n1.28\n1.44\n2.09\n▇▅▂▁▁\n\n\nlow_prec_freq\n0\n1.00\n254.65\n35.12\n169.90\n232.70\n255.85\n278.92\n348.70\n▂▅▇▅▁\n\n\nlow_prec_dur\n0\n1.00\n5.95\n3.20\n2.79\n4.24\n4.95\n6.70\n36.51\n▇▁▁▁▁\n\n\nglim_1st_class_frac\n0\n1.00\n0.79\n0.20\n0.30\n0.61\n0.83\n1.00\n1.00\n▁▃▃▃▇\n\n\nglim_2nd_class_frac\n0\n1.00\n0.16\n0.14\n0.00\n0.00\n0.14\n0.27\n0.49\n▇▃▃▂▁\n\n\ncarbonate_rocks_frac\n0\n1.00\n0.12\n0.26\n0.00\n0.00\n0.00\n0.04\n1.00\n▇▁▁▁▁\n\n\ngeol_porostiy\n3\n1.00\n0.13\n0.07\n0.01\n0.07\n0.13\n0.19\n0.28\n▇▆▇▇▂\n\n\ngeol_permeability\n0\n1.00\n-13.89\n1.18\n-16.50\n-14.77\n-13.96\n-13.00\n-10.90\n▂▅▇▅▂\n\n\nsoil_depth_pelletier\n0\n1.00\n10.87\n16.24\n0.27\n1.00\n1.23\n12.89\n50.00\n▇▁▁▁▁\n\n\nsoil_depth_statsgo\n0\n1.00\n1.29\n0.27\n0.40\n1.11\n1.46\n1.50\n1.50\n▁▁▂▂▇\n\n\nsoil_porosity\n0\n1.00\n0.44\n0.02\n0.37\n0.43\n0.44\n0.46\n0.68\n▃▇▁▁▁\n\n\nsoil_conductivity\n0\n1.00\n1.74\n1.52\n0.45\n0.93\n1.35\n1.93\n13.96\n▇▁▁▁▁\n\n\nmax_water_content\n0\n1.00\n0.53\n0.15\n0.09\n0.43\n0.56\n0.64\n1.05\n▁▅▇▃▁\n\n\nsand_frac\n0\n1.00\n36.47\n15.63\n8.18\n25.44\n35.27\n44.46\n91.98\n▅▇▅▁▁\n\n\nsilt_frac\n0\n1.00\n33.86\n13.25\n2.99\n23.95\n34.06\n43.64\n67.77\n▂▆▇▆▁\n\n\nclay_frac\n0\n1.00\n19.89\n9.32\n1.85\n14.00\n18.66\n25.42\n50.35\n▃▇▅▂▁\n\n\nwater_frac\n0\n1.00\n0.10\n0.94\n0.00\n0.00\n0.00\n0.00\n19.35\n▇▁▁▁▁\n\n\norganic_frac\n0\n1.00\n0.59\n3.84\n0.00\n0.00\n0.00\n0.00\n57.86\n▇▁▁▁▁\n\n\nother_frac\n0\n1.00\n9.82\n16.83\n0.00\n0.00\n1.31\n11.74\n99.38\n▇▁▁▁▁\n\n\ngauge_lat\n0\n1.00\n39.24\n5.21\n27.05\n35.70\n39.25\n43.21\n48.82\n▂▃▇▆▅\n\n\ngauge_lon\n0\n1.00\n-95.79\n16.21\n-124.39\n-110.41\n-92.78\n-81.77\n-67.94\n▆▃▇▇▅\n\n\nelev_mean\n0\n1.00\n759.42\n786.00\n10.21\n249.67\n462.72\n928.88\n3571.18\n▇▂▁▁▁\n\n\nslope_mean\n0\n1.00\n46.20\n47.12\n0.82\n7.43\n28.80\n73.17\n255.69\n▇▂▂▁▁\n\n\narea_gages2\n0\n1.00\n792.62\n1701.95\n4.03\n122.28\n329.68\n794.30\n25791.04\n▇▁▁▁▁\n\n\narea_geospa_fabric\n0\n1.00\n808.08\n1709.85\n4.10\n127.98\n340.70\n804.50\n25817.78\n▇▁▁▁▁\n\n\nfrac_forest\n0\n1.00\n0.64\n0.37\n0.00\n0.28\n0.81\n0.97\n1.00\n▃▁▁▂▇\n\n\nlai_max\n0\n1.00\n3.22\n1.52\n0.37\n1.81\n3.37\n4.70\n5.58\n▅▆▃▅▇\n\n\nlai_diff\n0\n1.00\n2.45\n1.33\n0.15\n1.20\n2.34\n3.76\n4.83\n▇▇▇▆▇\n\n\ngvf_max\n0\n1.00\n0.72\n0.17\n0.18\n0.61\n0.78\n0.86\n0.92\n▁▁▂▃▇\n\n\ngvf_diff\n0\n1.00\n0.32\n0.15\n0.03\n0.19\n0.32\n0.46\n0.65\n▃▇▅▇▁\n\n\ndom_land_cover_frac\n0\n1.00\n0.81\n0.18\n0.31\n0.65\n0.86\n1.00\n1.00\n▁▂▃▃▇\n\n\nroot_depth_50\n24\n0.96\n0.18\n0.03\n0.12\n0.17\n0.18\n0.19\n0.25\n▃▃▇▂▂\n\n\nroot_depth_99\n24\n0.96\n1.83\n0.30\n1.50\n1.52\n1.80\n2.00\n3.10\n▇▃▂▁▁\n\n\nq_mean\n1\n1.00\n1.49\n1.54\n0.00\n0.63\n1.13\n1.75\n9.69\n▇▁▁▁▁\n\n\nrunoff_ratio\n1\n1.00\n0.39\n0.23\n0.00\n0.24\n0.35\n0.51\n1.36\n▆▇▂▁▁\n\n\nslope_fdc\n1\n1.00\n1.24\n0.51\n0.00\n0.90\n1.28\n1.63\n2.50\n▂▅▇▇▁\n\n\nbaseflow_index\n0\n1.00\n0.49\n0.16\n0.01\n0.40\n0.50\n0.60\n0.98\n▁▃▇▅▁\n\n\nstream_elas\n1\n1.00\n1.83\n0.78\n-0.64\n1.32\n1.70\n2.23\n6.24\n▁▇▃▁▁\n\n\nq5\n1\n1.00\n0.17\n0.27\n0.00\n0.01\n0.08\n0.22\n2.42\n▇▁▁▁▁\n\n\nq95\n1\n1.00\n5.06\n4.94\n0.00\n2.07\n3.77\n6.29\n31.82\n▇▂▁▁▁\n\n\nhigh_q_freq\n1\n1.00\n25.74\n29.07\n0.00\n6.41\n15.10\n35.79\n172.80\n▇▂▁▁▁\n\n\nhigh_q_dur\n1\n1.00\n6.91\n10.07\n0.00\n1.82\n2.85\n7.55\n92.56\n▇▁▁▁▁\n\n\nlow_q_freq\n1\n1.00\n107.62\n82.24\n0.00\n37.44\n96.00\n162.14\n356.80\n▇▆▅▂▁\n\n\nlow_q_dur\n1\n1.00\n22.28\n21.66\n0.00\n10.00\n15.52\n26.91\n209.88\n▇▁▁▁▁\n\n\nzero_q_freq\n1\n1.00\n0.03\n0.11\n0.00\n0.00\n0.00\n0.00\n0.97\n▇▁▁▁▁\n\n\nhfd_mean\n1\n1.00\n182.52\n33.53\n112.25\n160.16\n173.77\n204.05\n287.75\n▂▇▃▂▁\n\n\nlogQmean\n1\n1.00\n-0.11\n1.17\n-5.39\n-0.46\n0.12\n0.56\n2.27\n▁▁▂▇▂\n\n\n\n\n#in the last column, skim gives you a general histogram shape\n\n52/58 columns are numeric and there are 671 total observations, well within the range for visdat. Skim is useful if we’re concered about the size of a dataset\n\n\nVisualization\n\nvis_dat(camels)\n\n\n\n\n\n\n\n\nMost of the missing values come from geol_2nd_class and root_depth_99, which we don’t really care about for streamflow. Looks like a pretty complete dataset!\n\n\nData Summary\n\nsummary(camels)\n\n      ...1         gauge_id             p_mean          pet_mean    \n Min.   :  1.0   Length:671         Min.   :0.6446   Min.   :1.899  \n 1st Qu.:168.5   Class :character   1st Qu.:2.3731   1st Qu.:2.335  \n Median :336.0   Mode  :character   Median :3.2295   Median :2.688  \n Mean   :336.0                      Mean   :3.2577   Mean   :2.787  \n 3rd Qu.:503.5                      3rd Qu.:3.7835   3rd Qu.:3.146  \n Max.   :671.0                      Max.   :8.9369   Max.   :4.744  \n                                                                    \n p_seasonality        frac_snow          aridity       high_prec_freq \n Min.   :-1.43546   Min.   :0.00000   Min.   :0.2203   Min.   : 7.90  \n 1st Qu.:-0.26352   1st Qu.:0.03514   1st Qu.:0.6957   1st Qu.:18.50  \n Median : 0.08093   Median :0.09793   Median :0.8551   Median :22.00  \n Mean   :-0.04128   Mean   :0.17760   Mean   :1.0565   Mean   :20.93  \n 3rd Qu.: 0.22399   3rd Qu.:0.22306   3rd Qu.:1.2673   3rd Qu.:24.23  \n Max.   : 0.92202   Max.   :0.90633   Max.   :5.2079   Max.   :32.70  \n                                                                      \n high_prec_dur   high_prec_timing   low_prec_freq    low_prec_dur   \n Min.   :1.075   Length:671         Min.   :169.9   Min.   : 2.789  \n 1st Qu.:1.209   Class :character   1st Qu.:232.7   1st Qu.: 4.241  \n Median :1.282   Mode  :character   Median :255.8   Median : 4.950  \n Mean   :1.350                      Mean   :254.6   Mean   : 5.954  \n 3rd Qu.:1.440                      3rd Qu.:278.9   3rd Qu.: 6.702  \n Max.   :2.091                      Max.   :348.7   Max.   :36.513  \n                                                                    \n low_prec_timing    geol_1st_class     glim_1st_class_frac geol_2nd_class    \n Length:671         Length:671         Min.   :0.2967      Length:671        \n Class :character   Class :character   1st Qu.:0.6083      Class :character  \n Mode  :character   Mode  :character   Median :0.8294      Mode  :character  \n                                       Mean   :0.7855                        \n                                       3rd Qu.:0.9971                        \n                                       Max.   :1.0000                        \n                                                                             \n glim_2nd_class_frac carbonate_rocks_frac geol_porostiy     geol_permeability\n Min.   :0.000000    Min.   :0.00000      Min.   :0.01000   Min.   :-16.50   \n 1st Qu.:0.002894    1st Qu.:0.00000      1st Qu.:0.06767   1st Qu.:-14.77   \n Median :0.136540    Median :0.00000      Median :0.13190   Median :-13.96   \n Mean   :0.155426    Mean   :0.11874      Mean   :0.12637   Mean   :-13.89   \n 3rd Qu.:0.266373    3rd Qu.:0.04333      3rd Qu.:0.18623   3rd Qu.:-13.00   \n Max.   :0.489930    Max.   :1.00000      Max.   :0.28000   Max.   :-10.90   \n                                          NA's   :3                          \n soil_depth_pelletier soil_depth_statsgo soil_porosity    soil_conductivity\n Min.   : 0.2667      Min.   :0.3999     Min.   :0.3733   Min.   : 0.4469  \n 1st Qu.: 1.0000      1st Qu.:1.1054     1st Qu.:0.4309   1st Qu.: 0.9321  \n Median : 1.2283      Median :1.4577     Median :0.4422   Median : 1.3477  \n Mean   :10.8728      Mean   :1.2932     Mean   :0.4426   Mean   : 1.7405  \n 3rd Qu.:12.8894      3rd Qu.:1.5000     3rd Qu.:0.4554   3rd Qu.: 1.9323  \n Max.   :50.0000      Max.   :1.5000     Max.   :0.6800   Max.   :13.9557  \n                                                                           \n max_water_content   sand_frac        silt_frac        clay_frac     \n Min.   :0.0866    Min.   : 8.184   Min.   : 2.985   Min.   : 1.846  \n 1st Qu.:0.4293    1st Qu.:25.437   1st Qu.:23.947   1st Qu.:13.999  \n Median :0.5579    Median :35.269   Median :34.059   Median :18.663  \n Mean   :0.5280    Mean   :36.468   Mean   :33.859   Mean   :19.886  \n 3rd Qu.:0.6450    3rd Qu.:44.457   3rd Qu.:43.639   3rd Qu.:25.420  \n Max.   :1.0520    Max.   :91.976   Max.   :67.775   Max.   :50.354  \n                                                                     \n   water_frac       organic_frac       other_frac       gauge_lat    \n Min.   : 0.0000   Min.   : 0.0000   Min.   : 0.000   Min.   :27.05  \n 1st Qu.: 0.0000   1st Qu.: 0.0000   1st Qu.: 0.000   1st Qu.:35.70  \n Median : 0.0000   Median : 0.0000   Median : 1.309   Median :39.25  \n Mean   : 0.1017   Mean   : 0.5918   Mean   : 9.825   Mean   :39.24  \n 3rd Qu.: 0.0000   3rd Qu.: 0.0000   3rd Qu.:11.737   3rd Qu.:43.21  \n Max.   :19.3545   Max.   :57.8631   Max.   :99.378   Max.   :48.82  \n                                                                     \n   gauge_lon         elev_mean         slope_mean        area_gages2      \n Min.   :-124.39   Min.   :  10.21   Min.   :  0.8222   Min.   :    4.03  \n 1st Qu.:-110.41   1st Qu.: 249.67   1st Qu.:  7.4268   1st Qu.:  122.28  \n Median : -92.78   Median : 462.72   Median : 28.8016   Median :  329.68  \n Mean   : -95.79   Mean   : 759.42   Mean   : 46.1953   Mean   :  792.62  \n 3rd Qu.: -81.77   3rd Qu.: 928.88   3rd Qu.: 73.1695   3rd Qu.:  794.29  \n Max.   : -67.94   Max.   :3571.18   Max.   :255.6884   Max.   :25791.04  \n                                                                          \n area_geospa_fabric  frac_forest        lai_max          lai_diff     \n Min.   :    4.1    Min.   :0.0000   Min.   :0.3671   Min.   :0.1544  \n 1st Qu.:  128.0    1st Qu.:0.2771   1st Qu.:1.8143   1st Qu.:1.1968  \n Median :  340.7    Median :0.8137   Median :3.3713   Median :2.3365  \n Mean   :  808.1    Mean   :0.6395   Mean   :3.2160   Mean   :2.4486  \n 3rd Qu.:  804.5    3rd Qu.:0.9724   3rd Qu.:4.6963   3rd Qu.:3.7574  \n Max.   :25817.8    Max.   :1.0000   Max.   :5.5821   Max.   :4.8315  \n                                                                      \n    gvf_max          gvf_diff      dom_land_cover_frac dom_land_cover    \n Min.   :0.1843   Min.   :0.0290   Min.   :0.3145      Length:671        \n 1st Qu.:0.6086   1st Qu.:0.1883   1st Qu.:0.6511      Class :character  \n Median :0.7803   Median :0.3160   Median :0.8582      Mode  :character  \n Mean   :0.7221   Mean   :0.3227   Mean   :0.8100                        \n 3rd Qu.:0.8649   3rd Qu.:0.4627   3rd Qu.:0.9967                        \n Max.   :0.9157   Max.   :0.6522   Max.   :1.0000                        \n                                                                         \n root_depth_50    root_depth_99       q_mean          runoff_ratio     \n Min.   :0.1200   Min.   :1.500   Min.   :0.004553   Min.   :0.004238  \n 1st Qu.:0.1654   1st Qu.:1.522   1st Qu.:0.632918   1st Qu.:0.242443  \n Median :0.1800   Median :1.800   Median :1.131817   Median :0.350664  \n Mean   :0.1788   Mean   :1.830   Mean   :1.493967   Mean   :0.387146  \n 3rd Qu.:0.1900   3rd Qu.:2.000   3rd Qu.:1.750901   3rd Qu.:0.506681  \n Max.   :0.2500   Max.   :3.100   Max.   :9.688438   Max.   :1.362132  \n NA's   :24       NA's   :24      NA's   :1          NA's   :1         \n   slope_fdc      baseflow_index      stream_elas            q5          \n Min.   :0.0000   Min.   :0.006858   Min.   :-0.6363   Min.   :0.000000  \n 1st Qu.:0.8978   1st Qu.:0.397430   1st Qu.: 1.3177   1st Qu.:0.009155  \n Median :1.2829   Median :0.504923   Median : 1.7006   Median :0.081568  \n Mean   :1.2372   Mean   :0.491447   Mean   : 1.8322   Mean   :0.171803  \n 3rd Qu.:1.6306   3rd Qu.:0.600345   3rd Qu.: 2.2255   3rd Qu.:0.219522  \n Max.   :2.4973   Max.   :0.977555   Max.   : 6.2405   Max.   :2.424938  \n NA's   :1                           NA's   :1         NA's   :1         \n      q95          high_q_freq        high_q_dur       low_q_freq    \n Min.   : 0.000   Min.   :  0.000   Min.   : 0.000   Min.   :  0.00  \n 1st Qu.: 2.066   1st Qu.:  6.412   1st Qu.: 1.821   1st Qu.: 37.44  \n Median : 3.769   Median : 15.100   Median : 2.848   Median : 96.00  \n Mean   : 5.057   Mean   : 25.745   Mean   : 6.913   Mean   :107.62  \n 3rd Qu.: 6.288   3rd Qu.: 35.788   3rd Qu.: 7.554   3rd Qu.:162.14  \n Max.   :31.817   Max.   :172.800   Max.   :92.559   Max.   :356.80  \n NA's   :1        NA's   :1         NA's   :1        NA's   :1       \n   low_q_dur       zero_q_freq         hfd_mean        logQmean      \n Min.   :  0.00   Min.   :0.00000   Min.   :112.2   Min.   :-5.3919  \n 1st Qu.: 10.00   1st Qu.:0.00000   1st Qu.:160.2   1st Qu.:-0.4574  \n Median : 15.52   Median :0.00000   Median :173.8   Median : 0.1238  \n Mean   : 22.28   Mean   :0.03415   Mean   :182.5   Mean   :-0.1063  \n 3rd Qu.: 26.91   3rd Qu.:0.00000   3rd Qu.:204.1   3rd Qu.: 0.5601  \n Max.   :209.88   Max.   :0.96537   Max.   :287.8   Max.   : 2.2709  \n NA's   :1        NA's   :1         NA's   :1       NA's   :1        \n\n\nSince the median mean streamflow (q_mean) is lower than the mean streamflow, there are probably spikes of much higher streamflow interspersed with longer periods of low flow. The range of mean streamflow is between .004 mm/day (basically nothing) and 9.68mm/day. Mean precipitation p_mean is close to the median, at 3.25 and 3.22, respectively. The minimum is .6446 while the maximum is 8.94. These ranges all seem reasonable."
  },
  {
    "objectID": "hyperparameter-tuning.html#question-how-do-i-automate-each-shapiro-test-so-that-all-the-variables-are-in-the-same-table",
    "href": "hyperparameter-tuning.html#question-how-do-i-automate-each-shapiro-test-so-that-all-the-variables-are-in-the-same-table",
    "title": "Machine Learning Pipline for Regression",
    "section": "Question: How do I automate each shapiro test so that all the variables are in the same table?",
    "text": "Question: How do I automate each shapiro test so that all the variables are in the same table?\nFor now, I am not going to do a shapiro test because it’s unlikely a model with all the inputs I’m about to add is linear. Instead, I will do a correlation test to see if I need to add interaction terms. Here is a list of the variables I will be selecting:\n\naridity\np_mean\nhigh_prec_freq\nhigh_prec_dur\nlow_prec_freq\nlai_max\nwater_frac\nslope_mean\n\n\nNow, I’m going to make a correlation matrix for these variables to see in interaction terms are necessary\n\ncamels |&gt; \n  select(q_mean, aridity, p_mean, high_prec_freq, high_prec_dur, low_prec_freq, lai_max, water_frac, slope_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n                    q_mean     aridity     p_mean high_prec_freq high_prec_dur\nq_mean          1.00000000 -0.58177710  0.8865757    -0.66875889   0.095663663\naridity        -0.58177710  1.00000000 -0.7550090     0.50924192   0.416194434\np_mean          0.88657569 -0.75500903  1.0000000    -0.60030682  -0.109246644\nhigh_prec_freq -0.66875889  0.50924192 -0.6003068     1.00000000   0.164939334\nhigh_prec_dur   0.09566366  0.41619443 -0.1092466     0.16493933   1.000000000\nlow_prec_freq  -0.71457114  0.74178481 -0.7252344     0.87176299   0.349279796\nlai_max         0.37907296 -0.70476393  0.5878266    -0.36234645  -0.478422889\nwater_frac     -0.03646015  0.03283288 -0.0663226    -0.03122678  -0.006140105\nslope_mean      0.51510689  0.01555387  0.2602688    -0.47146724   0.480449151\n               low_prec_freq     lai_max   water_frac  slope_mean\nq_mean            -0.7145711  0.37907296 -0.036460152  0.51510689\naridity            0.7417848 -0.70476393  0.032832877  0.01555387\np_mean            -0.7252344  0.58782662 -0.066322596  0.26026878\nhigh_prec_freq     0.8717630 -0.36234645 -0.031226777 -0.47146724\nhigh_prec_dur      0.3492798 -0.47842289 -0.006140105  0.48044915\nlow_prec_freq      1.0000000 -0.59053772 -0.015447702 -0.29491106\nlai_max           -0.5905377  1.00000000 -0.049832438 -0.11112101\nwater_frac        -0.0154477 -0.04983244  1.000000000 -0.01399994\nslope_mean        -0.2949111 -0.11112101 -0.013999942  1.00000000\n\n\nThe following have strong correlations:\n\nq_mean and low_prec_freq have a strong negative correlation\nq_mean and p_mean have a strong positive correlation\naridity and p_mean have strong negative correlations\naridity and low_prec_freq have strong positive correlations\naridity and lai_max have strong negative correlations\np_mean and low_prec_freq have strong positive correlations\nhigh_prec_freq and low_prec_freq have strong positive correlations\n\n… so for these pairs we will make interaction terms in the recipe.\nfor all of the variables that have strong correlations, there are many more that have weaker ones, so we will continue with nonparametric modeling.\n\n\nOne last strategy to see if we can normalize the data. From the last column in skimr, I could see that none of the predictor variables are normally distributed. Now, let’s test the logged version of each variable.\n\ncamelog &lt;- camels %&gt;% \n  mutate(logarid = log(aridity),\n         logp_mean = log(p_mean),\n         logprecfreq = log(high_prec_freq),\n         logprecdur = log(high_prec_dur),\n         loglowprecfreq = log(low_prec_freq),\n         loglaimax = log(lai_max),\n         logwaterfrac = log(water_frac),\n         logslopemean = log(slope_mean)) %&gt;% \n  select(!c(aridity, p_mean, high_prec_freq, high_prec_dur, low_prec_freq, lai_max, water_frac, slope_mean, p_seasonality, frac_snow, high_prec_timing, low_prec_dur, geol_1st_class, glim_1st_class_frac, geol_2nd_class, glim_2nd_class_frac, carbonate_rocks_frac, geol_porostiy, geol_permeability, soil_depth_pelletier, soil_depth_statsgo, soil_porosity, soil_conductivity, max_water_content, sand_frac, silt_frac, clay_frac, organic_frac, other_frac, lai_diff, gvf_max, gvf_diff, dom_land_cover_frac, dom_land_cover, root_depth_50, root_depth_99, q_mean, runoff_ratio, slope_fdc, baseflow_index, stream_elas, q5, q95, high_q_freq, high_q_dur, low_q_dur, zero_q_freq, hfd_mean))\n\n\nChecking normality in logged variables\n\nshapiro.test(camelog$logarid)\n\n\n    Shapiro-Wilk normality test\n\ndata:  camelog$logarid\nW = 0.97302, p-value = 8.807e-10\n\ngghistogram(camelog$logarid)\n\nWarning: Using `bins = 30` by default. Pick better value with the argument\n`bins`.\n\n\n\n\n\n\n\n\n\nAll of this logging with no normal distributions further supports my decision to use models that don’t assume normal distribution, especially when using so many predictor variables.\n\n\n\nTraining and Testing Data\n\nset.seed(225)\n\ncamels_split3 &lt;- initial_split(camels, prop = 0.8)\ncamels_train3 &lt;- training(camels_split3)\ncamels_test3  &lt;- testing(camels_split3)\n\ncamels_cv3 &lt;- vfold_cv(camels_train3, v = 10)"
  },
  {
    "objectID": "hyperparameter-tuning.html#the-recipe-after-checking-all-predictor-variables",
    "href": "hyperparameter-tuning.html#the-recipe-after-checking-all-predictor-variables",
    "title": "Machine Learning Pipline for Regression",
    "section": "The recipe after checking all predictor variables",
    "text": "The recipe after checking all predictor variables\n\npredictors to try in the recipe\n\naridity\np_mean\nhigh_prec_freq\nhigh_prec_dur\nlow_prec_freq\nlai_max\nwater_frac\nslope_mean\n\n\n\nInteraction terms to add to the recipe\n\nq_mean:low_prec_freq\nq_mean:p_mean\naridity:p_mean\naridity:low_prec_freq\naridity:lai_max\np_mean:low_prec_freq\nhigh_prec_freq:low_prec_freq\n\n\nrec3 &lt;-  recipe(q_mean ~ aridity + p_mean + high_prec_freq + low_prec_freq + high_prec_dur + lai_max + water_frac + slope_mean, data = camels_train3) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  #step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean + aridity:low_prec_freq +\n                          aridity:lai_max + p_mean:low_prec_freq +\n                           high_prec_freq:low_prec_freq) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\nThe goal of the modelbuilding is still to predict q_mean, so how is this different from lab 6?"
  },
  {
    "objectID": "hyperparameter-tuning.html#building-candidate-models",
    "href": "hyperparameter-tuning.html#building-candidate-models",
    "title": "Machine Learning Pipline for Regression",
    "section": "Building Candidate Models",
    "text": "Building Candidate Models\n\n#Random Forest Model\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n#Neural Network Model\nlibrary(baguette)\nlibrary(xgboost)\n\n\nAttaching package: 'xgboost'\n\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\nnnet_model &lt;- bag_mlp() %&gt;% \n  set_engine(\"nnet\") %&gt;% \n  set_mode(\"regression\")\n\n#Xgboost Model\nlibrary(parsnip)\nxgboost_model &lt;- boost_tree() %&gt;% \n  set_engine(\"xgboost\") %&gt;% \n  set_mode(\"regression\")\n\n#linear regression model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n\nwf3 &lt;- workflow_set(list(rec3), list(lm_model, rf_model, nnet_model, xgboost_model)) %&gt;%\n#map models to recipes\n    workflow_map('fit_resamples', resamples = camels_cv3) \n\nautoplot(wf3)\n\n\n\n\n\n\n\nrank_results(wf3, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.417  0.0449    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.922  0.0138    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.413  0.0400    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.919  0.0149    10 recipe       line…     2\n5 recipe_boost_tree Prepro… rmse    0.467  0.0475    10 recipe       boos…     3\n6 recipe_boost_tree Prepro… rsq     0.906  0.0160    10 recipe       boos…     3\n7 recipe_bag_mlp    Prepro… rmse    1.00   0.0520    10 recipe       bag_…     4\n8 recipe_bag_mlp    Prepro… rsq     0.769  0.0272    10 recipe       bag_…     4"
  },
  {
    "objectID": "hyperparameter-tuning.html#select-a-model-you-think-best-performs",
    "href": "hyperparameter-tuning.html#select-a-model-you-think-best-performs",
    "title": "Machine Learning Pipline for Regression",
    "section": "4. Select a model you think best performs",
    "text": "4. Select a model you think best performs\nAt an r-squared of .922, the random forest model beats out the competition. Therefore, I select this model, especially because there’s no requirement for explainability.\nrf_model &lt;- rand_forest() %&gt;% set_engine(“ranger”, importance = “impurity”) %&gt;% set_mode(“regression”)\nThe engine is ranger, which creates a large amount of decision trees, and the mode is regression, which predicts continuous values such as streamflow. This combination is performing well because having more decision trees, which can reduce variance and reduce the risk of overfitting, which is what happens when the model learns noisy data like streamflow too well."
  },
  {
    "objectID": "hyperparameter-tuning.html#chosen-model-with-hyperparameter-specifications",
    "href": "hyperparameter-tuning.html#chosen-model-with-hyperparameter-specifications",
    "title": "Machine Learning Pipline for Regression",
    "section": "Chosen Model with Hyperparameter Specifications",
    "text": "Chosen Model with Hyperparameter Specifications\nWhat are hyperparameters? Hyperparameters are settings that control the learning process of a model. They are set before training and affect the model’s performance. Hyperparameters can be tuned to optimize the model’s predictive power.\n\n# I'm sticking with ranger and regression\n\nrf_model_tune &lt;- rand_forest(\n  mtry = tune(),\n  trees = tune(), \n  min_n = tune()\n) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\", seed = 123) %&gt;%\n  set_mode(\"regression\")"
  },
  {
    "objectID": "hyperparameter-tuning.html#create-a-workflow-from-chosen-model",
    "href": "hyperparameter-tuning.html#create-a-workflow-from-chosen-model",
    "title": "Machine Learning Pipline for Regression",
    "section": "Create a workflow from chosen model",
    "text": "Create a workflow from chosen model\n\nrf_wf3_tune &lt;- workflow() %&gt;%\n  add_recipe(rec3) %&gt;%\n  add_model(rf_model_tune)"
  },
  {
    "objectID": "hyperparameter-tuning.html#check-tunable-valuesranges",
    "href": "hyperparameter-tuning.html#check-tunable-valuesranges",
    "title": "Machine Learning Pipline for Regression",
    "section": "Check Tunable Values/Ranges",
    "text": "Check Tunable Values/Ranges\n\ndials_params &lt;- extract_parameter_set_dials(rf_wf3_tune) %&gt;% \n  update(\n    mtry = mtry(range = c(1, 10)),\n    trees = trees(range = c(200, 800)),\n    min_n = min_n(range = c(5, 15))\n  ) %&gt;%\n  finalize(x = camels_train3 %&gt;% select(-q_mean))\n\n# Verify parameters\ndials_params$object\n\n[[1]]\n\n\n# Randomly Selected Predictors (quantitative)\n\n\nRange: [1, 10]\n\n\n\n[[2]]\n\n\n# Trees (quantitative)\n\n\nRange: [200, 800]\n\n\n\n[[3]]\n\n\nMinimal Node Size (quantitative)\n\n\nRange: [5, 15]\n\n\n\nprint(dials_params)\n\nCollection of 3 parameters for tuning\n\n\n\n\n\n identifier  type    object\n       mtry  mtry nparam[+]\n      trees trees nparam[+]\n      min_n min_n nparam[+]\n\n\n\n\n\n\nset.seed(123)\n\nmy.grid &lt;- grid_latin_hypercube(dials_params,\n  size = 20\n)\n\nWarning: `grid_latin_hypercube()` was deprecated in dials 1.3.0.\nℹ Please use `grid_space_filling()` instead.\n\nprint(my.grid)\n\n# A tibble: 20 × 3\n    mtry trees min_n\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     8   242    13\n 2     6   714    11\n 3     7   344    10\n 4    10   685     8\n 5     5   529    14\n 6     9   429    11\n 7     2   560     5\n 8     1   539     6\n 9     5   488     9\n10     8   308     8\n11     2   376     6\n12     3   592    14\n13     3   396    12\n14     4   455     9\n15     6   742    13\n16     5   227     7\n17     6   659    12\n18     8   287    10\n19     9   645     7\n20     3   792    15"
  },
  {
    "objectID": "hyperparameter-tuning.html#tuning-the-model",
    "href": "hyperparameter-tuning.html#tuning-the-model",
    "title": "Machine Learning Pipline for Regression",
    "section": "Tuning the Model",
    "text": "Tuning the Model\n\nmodel_params &lt;- tune_grid(\n  rf_wf3_tune,          # Your tuned workflow\n  resamples = camels_cv3,  # Your cross-validation folds (previously created)\n  grid = my.grid,       # Your parameter grid\n  metrics = metric_set(rmse, rsq, mae),  # Evaluation metrics\n  control = control_grid(\n    save_pred = TRUE,   # Save predictions for analysis\n    verbose = TRUE      # Show progress\n  )\n)\n\ni Fold01: preprocessor 1/1\n\n\n✓ Fold01: preprocessor 1/1\n\n\ni Fold01: preprocessor 1/1, model 1/20\n\n\n✓ Fold01: preprocessor 1/1, model 1/20\n\n\ni Fold01: preprocessor 1/1, model 1/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 1/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 2/20\n\n\n✓ Fold01: preprocessor 1/1, model 2/20\n\n\ni Fold01: preprocessor 1/1, model 2/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 2/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 3/20\n\n\n✓ Fold01: preprocessor 1/1, model 3/20\n\n\ni Fold01: preprocessor 1/1, model 3/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 3/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 4/20\n\n\n✓ Fold01: preprocessor 1/1, model 4/20\n\n\ni Fold01: preprocessor 1/1, model 4/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 4/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 5/20\n\n\n✓ Fold01: preprocessor 1/1, model 5/20\n\n\ni Fold01: preprocessor 1/1, model 5/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 5/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 6/20\n\n\n✓ Fold01: preprocessor 1/1, model 6/20\n\n\ni Fold01: preprocessor 1/1, model 6/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 6/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 7/20\n\n\n✓ Fold01: preprocessor 1/1, model 7/20\n\n\ni Fold01: preprocessor 1/1, model 7/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 7/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 8/20\n\n\n✓ Fold01: preprocessor 1/1, model 8/20\n\n\ni Fold01: preprocessor 1/1, model 8/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 8/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 9/20\n\n\n✓ Fold01: preprocessor 1/1, model 9/20\n\n\ni Fold01: preprocessor 1/1, model 9/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 9/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 10/20\n\n\n✓ Fold01: preprocessor 1/1, model 10/20\n\n\ni Fold01: preprocessor 1/1, model 10/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 10/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 11/20\n\n\n✓ Fold01: preprocessor 1/1, model 11/20\n\n\ni Fold01: preprocessor 1/1, model 11/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 11/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 12/20\n\n\n✓ Fold01: preprocessor 1/1, model 12/20\n\n\ni Fold01: preprocessor 1/1, model 12/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 12/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 13/20\n\n\n✓ Fold01: preprocessor 1/1, model 13/20\n\n\ni Fold01: preprocessor 1/1, model 13/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 13/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 14/20\n\n\n✓ Fold01: preprocessor 1/1, model 14/20\n\n\ni Fold01: preprocessor 1/1, model 14/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 14/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 15/20\n\n\n✓ Fold01: preprocessor 1/1, model 15/20\n\n\ni Fold01: preprocessor 1/1, model 15/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 15/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 16/20\n\n\n✓ Fold01: preprocessor 1/1, model 16/20\n\n\ni Fold01: preprocessor 1/1, model 16/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 16/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 17/20\n\n\n✓ Fold01: preprocessor 1/1, model 17/20\n\n\ni Fold01: preprocessor 1/1, model 17/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 17/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 18/20\n\n\n✓ Fold01: preprocessor 1/1, model 18/20\n\n\ni Fold01: preprocessor 1/1, model 18/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 18/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 19/20\n\n\n✓ Fold01: preprocessor 1/1, model 19/20\n\n\ni Fold01: preprocessor 1/1, model 19/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 19/20 (predictions)\n\n\ni Fold01: preprocessor 1/1, model 20/20\n\n\n✓ Fold01: preprocessor 1/1, model 20/20\n\n\ni Fold01: preprocessor 1/1, model 20/20 (extracts)\n\n\ni Fold01: preprocessor 1/1, model 20/20 (predictions)\n\n\ni Fold02: preprocessor 1/1\n\n\n✓ Fold02: preprocessor 1/1\n\n\ni Fold02: preprocessor 1/1, model 1/20\n\n\n✓ Fold02: preprocessor 1/1, model 1/20\n\n\ni Fold02: preprocessor 1/1, model 1/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 1/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 2/20\n\n\n✓ Fold02: preprocessor 1/1, model 2/20\n\n\ni Fold02: preprocessor 1/1, model 2/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 2/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 3/20\n\n\n✓ Fold02: preprocessor 1/1, model 3/20\n\n\ni Fold02: preprocessor 1/1, model 3/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 3/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 4/20\n\n\n✓ Fold02: preprocessor 1/1, model 4/20\n\n\ni Fold02: preprocessor 1/1, model 4/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 4/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 5/20\n\n\n✓ Fold02: preprocessor 1/1, model 5/20\n\n\ni Fold02: preprocessor 1/1, model 5/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 5/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 6/20\n\n\n✓ Fold02: preprocessor 1/1, model 6/20\n\n\ni Fold02: preprocessor 1/1, model 6/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 6/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 7/20\n\n\n✓ Fold02: preprocessor 1/1, model 7/20\n\n\ni Fold02: preprocessor 1/1, model 7/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 7/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 8/20\n\n\n✓ Fold02: preprocessor 1/1, model 8/20\n\n\ni Fold02: preprocessor 1/1, model 8/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 8/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 9/20\n\n\n✓ Fold02: preprocessor 1/1, model 9/20\n\n\ni Fold02: preprocessor 1/1, model 9/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 9/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 10/20\n\n\n✓ Fold02: preprocessor 1/1, model 10/20\n\n\ni Fold02: preprocessor 1/1, model 10/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 10/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 11/20\n\n\n✓ Fold02: preprocessor 1/1, model 11/20\n\n\ni Fold02: preprocessor 1/1, model 11/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 11/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 12/20\n\n\n✓ Fold02: preprocessor 1/1, model 12/20\n\n\ni Fold02: preprocessor 1/1, model 12/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 12/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 13/20\n\n\n✓ Fold02: preprocessor 1/1, model 13/20\n\n\ni Fold02: preprocessor 1/1, model 13/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 13/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 14/20\n\n\n✓ Fold02: preprocessor 1/1, model 14/20\n\n\ni Fold02: preprocessor 1/1, model 14/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 14/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 15/20\n\n\n✓ Fold02: preprocessor 1/1, model 15/20\n\n\ni Fold02: preprocessor 1/1, model 15/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 15/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 16/20\n\n\n✓ Fold02: preprocessor 1/1, model 16/20\n\n\ni Fold02: preprocessor 1/1, model 16/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 16/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 17/20\n\n\n✓ Fold02: preprocessor 1/1, model 17/20\n\n\ni Fold02: preprocessor 1/1, model 17/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 17/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 18/20\n\n\n✓ Fold02: preprocessor 1/1, model 18/20\n\n\ni Fold02: preprocessor 1/1, model 18/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 18/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 19/20\n\n\n✓ Fold02: preprocessor 1/1, model 19/20\n\n\ni Fold02: preprocessor 1/1, model 19/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 19/20 (predictions)\n\n\ni Fold02: preprocessor 1/1, model 20/20\n\n\n✓ Fold02: preprocessor 1/1, model 20/20\n\n\ni Fold02: preprocessor 1/1, model 20/20 (extracts)\n\n\ni Fold02: preprocessor 1/1, model 20/20 (predictions)\n\n\ni Fold03: preprocessor 1/1\n\n\n✓ Fold03: preprocessor 1/1\n\n\ni Fold03: preprocessor 1/1, model 1/20\n\n\n✓ Fold03: preprocessor 1/1, model 1/20\n\n\ni Fold03: preprocessor 1/1, model 1/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 1/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 2/20\n\n\n✓ Fold03: preprocessor 1/1, model 2/20\n\n\ni Fold03: preprocessor 1/1, model 2/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 2/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 3/20\n\n\n✓ Fold03: preprocessor 1/1, model 3/20\n\n\ni Fold03: preprocessor 1/1, model 3/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 3/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 4/20\n\n\n✓ Fold03: preprocessor 1/1, model 4/20\n\n\ni Fold03: preprocessor 1/1, model 4/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 4/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 5/20\n\n\n✓ Fold03: preprocessor 1/1, model 5/20\n\n\ni Fold03: preprocessor 1/1, model 5/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 5/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 6/20\n\n\n✓ Fold03: preprocessor 1/1, model 6/20\n\n\ni Fold03: preprocessor 1/1, model 6/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 6/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 7/20\n\n\n✓ Fold03: preprocessor 1/1, model 7/20\n\n\ni Fold03: preprocessor 1/1, model 7/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 7/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 8/20\n\n\n✓ Fold03: preprocessor 1/1, model 8/20\n\n\ni Fold03: preprocessor 1/1, model 8/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 8/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 9/20\n\n\n✓ Fold03: preprocessor 1/1, model 9/20\n\n\ni Fold03: preprocessor 1/1, model 9/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 9/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 10/20\n\n\n✓ Fold03: preprocessor 1/1, model 10/20\n\n\ni Fold03: preprocessor 1/1, model 10/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 10/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 11/20\n\n\n✓ Fold03: preprocessor 1/1, model 11/20\n\n\ni Fold03: preprocessor 1/1, model 11/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 11/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 12/20\n\n\n✓ Fold03: preprocessor 1/1, model 12/20\n\n\ni Fold03: preprocessor 1/1, model 12/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 12/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 13/20\n\n\n✓ Fold03: preprocessor 1/1, model 13/20\n\n\ni Fold03: preprocessor 1/1, model 13/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 13/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 14/20\n\n\n✓ Fold03: preprocessor 1/1, model 14/20\n\n\ni Fold03: preprocessor 1/1, model 14/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 14/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 15/20\n\n\n✓ Fold03: preprocessor 1/1, model 15/20\n\n\ni Fold03: preprocessor 1/1, model 15/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 15/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 16/20\n\n\n✓ Fold03: preprocessor 1/1, model 16/20\n\n\ni Fold03: preprocessor 1/1, model 16/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 16/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 17/20\n\n\n✓ Fold03: preprocessor 1/1, model 17/20\n\n\ni Fold03: preprocessor 1/1, model 17/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 17/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 18/20\n\n\n✓ Fold03: preprocessor 1/1, model 18/20\n\n\ni Fold03: preprocessor 1/1, model 18/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 18/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 19/20\n\n\n✓ Fold03: preprocessor 1/1, model 19/20\n\n\ni Fold03: preprocessor 1/1, model 19/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 19/20 (predictions)\n\n\ni Fold03: preprocessor 1/1, model 20/20\n\n\n✓ Fold03: preprocessor 1/1, model 20/20\n\n\ni Fold03: preprocessor 1/1, model 20/20 (extracts)\n\n\ni Fold03: preprocessor 1/1, model 20/20 (predictions)\n\n\ni Fold04: preprocessor 1/1\n\n\n✓ Fold04: preprocessor 1/1\n\n\ni Fold04: preprocessor 1/1, model 1/20\n\n\n✓ Fold04: preprocessor 1/1, model 1/20\n\n\ni Fold04: preprocessor 1/1, model 1/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 1/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 2/20\n\n\n✓ Fold04: preprocessor 1/1, model 2/20\n\n\ni Fold04: preprocessor 1/1, model 2/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 2/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 3/20\n\n\n✓ Fold04: preprocessor 1/1, model 3/20\n\n\ni Fold04: preprocessor 1/1, model 3/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 3/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 4/20\n\n\n✓ Fold04: preprocessor 1/1, model 4/20\n\n\ni Fold04: preprocessor 1/1, model 4/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 4/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 5/20\n\n\n✓ Fold04: preprocessor 1/1, model 5/20\n\n\ni Fold04: preprocessor 1/1, model 5/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 5/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 6/20\n\n\n✓ Fold04: preprocessor 1/1, model 6/20\n\n\ni Fold04: preprocessor 1/1, model 6/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 6/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 7/20\n\n\n✓ Fold04: preprocessor 1/1, model 7/20\n\n\ni Fold04: preprocessor 1/1, model 7/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 7/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 8/20\n\n\n✓ Fold04: preprocessor 1/1, model 8/20\n\n\ni Fold04: preprocessor 1/1, model 8/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 8/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 9/20\n\n\n✓ Fold04: preprocessor 1/1, model 9/20\n\n\ni Fold04: preprocessor 1/1, model 9/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 9/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 10/20\n\n\n✓ Fold04: preprocessor 1/1, model 10/20\n\n\ni Fold04: preprocessor 1/1, model 10/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 10/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 11/20\n\n\n✓ Fold04: preprocessor 1/1, model 11/20\n\n\ni Fold04: preprocessor 1/1, model 11/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 11/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 12/20\n\n\n✓ Fold04: preprocessor 1/1, model 12/20\n\n\ni Fold04: preprocessor 1/1, model 12/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 12/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 13/20\n\n\n✓ Fold04: preprocessor 1/1, model 13/20\n\n\ni Fold04: preprocessor 1/1, model 13/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 13/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 14/20\n\n\n✓ Fold04: preprocessor 1/1, model 14/20\n\n\ni Fold04: preprocessor 1/1, model 14/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 14/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 15/20\n\n\n✓ Fold04: preprocessor 1/1, model 15/20\n\n\ni Fold04: preprocessor 1/1, model 15/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 15/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 16/20\n\n\n✓ Fold04: preprocessor 1/1, model 16/20\n\n\ni Fold04: preprocessor 1/1, model 16/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 16/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 17/20\n\n\n✓ Fold04: preprocessor 1/1, model 17/20\n\n\ni Fold04: preprocessor 1/1, model 17/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 17/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 18/20\n\n\n✓ Fold04: preprocessor 1/1, model 18/20\n\n\ni Fold04: preprocessor 1/1, model 18/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 18/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 19/20\n\n\n✓ Fold04: preprocessor 1/1, model 19/20\n\n\ni Fold04: preprocessor 1/1, model 19/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 19/20 (predictions)\n\n\ni Fold04: preprocessor 1/1, model 20/20\n\n\n✓ Fold04: preprocessor 1/1, model 20/20\n\n\ni Fold04: preprocessor 1/1, model 20/20 (extracts)\n\n\ni Fold04: preprocessor 1/1, model 20/20 (predictions)\n\n\ni Fold05: preprocessor 1/1\n\n\n✓ Fold05: preprocessor 1/1\n\n\ni Fold05: preprocessor 1/1, model 1/20\n\n\n✓ Fold05: preprocessor 1/1, model 1/20\n\n\ni Fold05: preprocessor 1/1, model 1/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 1/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 2/20\n\n\n✓ Fold05: preprocessor 1/1, model 2/20\n\n\ni Fold05: preprocessor 1/1, model 2/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 2/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 3/20\n\n\n✓ Fold05: preprocessor 1/1, model 3/20\n\n\ni Fold05: preprocessor 1/1, model 3/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 3/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 4/20\n\n\n✓ Fold05: preprocessor 1/1, model 4/20\n\n\ni Fold05: preprocessor 1/1, model 4/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 4/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 5/20\n\n\n✓ Fold05: preprocessor 1/1, model 5/20\n\n\ni Fold05: preprocessor 1/1, model 5/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 5/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 6/20\n\n\n✓ Fold05: preprocessor 1/1, model 6/20\n\n\ni Fold05: preprocessor 1/1, model 6/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 6/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 7/20\n\n\n✓ Fold05: preprocessor 1/1, model 7/20\n\n\ni Fold05: preprocessor 1/1, model 7/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 7/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 8/20\n\n\n✓ Fold05: preprocessor 1/1, model 8/20\n\n\ni Fold05: preprocessor 1/1, model 8/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 8/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 9/20\n\n\n✓ Fold05: preprocessor 1/1, model 9/20\n\n\ni Fold05: preprocessor 1/1, model 9/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 9/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 10/20\n\n\n✓ Fold05: preprocessor 1/1, model 10/20\n\n\ni Fold05: preprocessor 1/1, model 10/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 10/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 11/20\n\n\n✓ Fold05: preprocessor 1/1, model 11/20\n\n\ni Fold05: preprocessor 1/1, model 11/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 11/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 12/20\n\n\n✓ Fold05: preprocessor 1/1, model 12/20\n\n\ni Fold05: preprocessor 1/1, model 12/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 12/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 13/20\n\n\n✓ Fold05: preprocessor 1/1, model 13/20\n\n\ni Fold05: preprocessor 1/1, model 13/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 13/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 14/20\n\n\n✓ Fold05: preprocessor 1/1, model 14/20\n\n\ni Fold05: preprocessor 1/1, model 14/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 14/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 15/20\n\n\n✓ Fold05: preprocessor 1/1, model 15/20\n\n\ni Fold05: preprocessor 1/1, model 15/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 15/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 16/20\n\n\n✓ Fold05: preprocessor 1/1, model 16/20\n\n\ni Fold05: preprocessor 1/1, model 16/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 16/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 17/20\n\n\n✓ Fold05: preprocessor 1/1, model 17/20\n\n\ni Fold05: preprocessor 1/1, model 17/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 17/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 18/20\n\n\n✓ Fold05: preprocessor 1/1, model 18/20\n\n\ni Fold05: preprocessor 1/1, model 18/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 18/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 19/20\n\n\n✓ Fold05: preprocessor 1/1, model 19/20\n\n\ni Fold05: preprocessor 1/1, model 19/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 19/20 (predictions)\n\n\ni Fold05: preprocessor 1/1, model 20/20\n\n\n✓ Fold05: preprocessor 1/1, model 20/20\n\n\ni Fold05: preprocessor 1/1, model 20/20 (extracts)\n\n\ni Fold05: preprocessor 1/1, model 20/20 (predictions)\n\n\ni Fold06: preprocessor 1/1\n\n\n✓ Fold06: preprocessor 1/1\n\n\ni Fold06: preprocessor 1/1, model 1/20\n\n\n✓ Fold06: preprocessor 1/1, model 1/20\n\n\ni Fold06: preprocessor 1/1, model 1/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 1/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 2/20\n\n\n✓ Fold06: preprocessor 1/1, model 2/20\n\n\ni Fold06: preprocessor 1/1, model 2/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 2/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 3/20\n\n\n✓ Fold06: preprocessor 1/1, model 3/20\n\n\ni Fold06: preprocessor 1/1, model 3/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 3/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 4/20\n\n\n✓ Fold06: preprocessor 1/1, model 4/20\n\n\ni Fold06: preprocessor 1/1, model 4/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 4/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 5/20\n\n\n✓ Fold06: preprocessor 1/1, model 5/20\n\n\ni Fold06: preprocessor 1/1, model 5/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 5/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 6/20\n\n\n✓ Fold06: preprocessor 1/1, model 6/20\n\n\ni Fold06: preprocessor 1/1, model 6/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 6/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 7/20\n\n\n✓ Fold06: preprocessor 1/1, model 7/20\n\n\ni Fold06: preprocessor 1/1, model 7/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 7/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 8/20\n\n\n✓ Fold06: preprocessor 1/1, model 8/20\n\n\ni Fold06: preprocessor 1/1, model 8/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 8/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 9/20\n\n\n✓ Fold06: preprocessor 1/1, model 9/20\n\n\ni Fold06: preprocessor 1/1, model 9/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 9/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 10/20\n\n\n✓ Fold06: preprocessor 1/1, model 10/20\n\n\ni Fold06: preprocessor 1/1, model 10/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 10/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 11/20\n\n\n✓ Fold06: preprocessor 1/1, model 11/20\n\n\ni Fold06: preprocessor 1/1, model 11/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 11/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 12/20\n\n\n✓ Fold06: preprocessor 1/1, model 12/20\n\n\ni Fold06: preprocessor 1/1, model 12/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 12/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 13/20\n\n\n✓ Fold06: preprocessor 1/1, model 13/20\n\n\ni Fold06: preprocessor 1/1, model 13/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 13/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 14/20\n\n\n✓ Fold06: preprocessor 1/1, model 14/20\n\n\ni Fold06: preprocessor 1/1, model 14/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 14/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 15/20\n\n\n✓ Fold06: preprocessor 1/1, model 15/20\n\n\ni Fold06: preprocessor 1/1, model 15/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 15/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 16/20\n\n\n✓ Fold06: preprocessor 1/1, model 16/20\n\n\ni Fold06: preprocessor 1/1, model 16/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 16/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 17/20\n\n\n✓ Fold06: preprocessor 1/1, model 17/20\n\n\ni Fold06: preprocessor 1/1, model 17/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 17/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 18/20\n\n\n✓ Fold06: preprocessor 1/1, model 18/20\n\n\ni Fold06: preprocessor 1/1, model 18/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 18/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 19/20\n\n\n✓ Fold06: preprocessor 1/1, model 19/20\n\n\ni Fold06: preprocessor 1/1, model 19/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 19/20 (predictions)\n\n\ni Fold06: preprocessor 1/1, model 20/20\n\n\n✓ Fold06: preprocessor 1/1, model 20/20\n\n\ni Fold06: preprocessor 1/1, model 20/20 (extracts)\n\n\ni Fold06: preprocessor 1/1, model 20/20 (predictions)\n\n\ni Fold07: preprocessor 1/1\n\n\n✓ Fold07: preprocessor 1/1\n\n\ni Fold07: preprocessor 1/1, model 1/20\n\n\n✓ Fold07: preprocessor 1/1, model 1/20\n\n\ni Fold07: preprocessor 1/1, model 1/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 1/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 2/20\n\n\n✓ Fold07: preprocessor 1/1, model 2/20\n\n\ni Fold07: preprocessor 1/1, model 2/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 2/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 3/20\n\n\n✓ Fold07: preprocessor 1/1, model 3/20\n\n\ni Fold07: preprocessor 1/1, model 3/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 3/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 4/20\n\n\n✓ Fold07: preprocessor 1/1, model 4/20\n\n\ni Fold07: preprocessor 1/1, model 4/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 4/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 5/20\n\n\n✓ Fold07: preprocessor 1/1, model 5/20\n\n\ni Fold07: preprocessor 1/1, model 5/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 5/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 6/20\n\n\n✓ Fold07: preprocessor 1/1, model 6/20\n\n\ni Fold07: preprocessor 1/1, model 6/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 6/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 7/20\n\n\n✓ Fold07: preprocessor 1/1, model 7/20\n\n\ni Fold07: preprocessor 1/1, model 7/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 7/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 8/20\n\n\n✓ Fold07: preprocessor 1/1, model 8/20\n\n\ni Fold07: preprocessor 1/1, model 8/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 8/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 9/20\n\n\n✓ Fold07: preprocessor 1/1, model 9/20\n\n\ni Fold07: preprocessor 1/1, model 9/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 9/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 10/20\n\n\n✓ Fold07: preprocessor 1/1, model 10/20\n\n\ni Fold07: preprocessor 1/1, model 10/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 10/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 11/20\n\n\n✓ Fold07: preprocessor 1/1, model 11/20\n\n\ni Fold07: preprocessor 1/1, model 11/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 11/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 12/20\n\n\n✓ Fold07: preprocessor 1/1, model 12/20\n\n\ni Fold07: preprocessor 1/1, model 12/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 12/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 13/20\n\n\n✓ Fold07: preprocessor 1/1, model 13/20\n\n\ni Fold07: preprocessor 1/1, model 13/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 13/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 14/20\n\n\n✓ Fold07: preprocessor 1/1, model 14/20\n\n\ni Fold07: preprocessor 1/1, model 14/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 14/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 15/20\n\n\n✓ Fold07: preprocessor 1/1, model 15/20\n\n\ni Fold07: preprocessor 1/1, model 15/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 15/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 16/20\n\n\n✓ Fold07: preprocessor 1/1, model 16/20\n\n\ni Fold07: preprocessor 1/1, model 16/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 16/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 17/20\n\n\n✓ Fold07: preprocessor 1/1, model 17/20\n\n\ni Fold07: preprocessor 1/1, model 17/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 17/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 18/20\n\n\n✓ Fold07: preprocessor 1/1, model 18/20\n\n\ni Fold07: preprocessor 1/1, model 18/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 18/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 19/20\n\n\n✓ Fold07: preprocessor 1/1, model 19/20\n\n\ni Fold07: preprocessor 1/1, model 19/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 19/20 (predictions)\n\n\ni Fold07: preprocessor 1/1, model 20/20\n\n\n✓ Fold07: preprocessor 1/1, model 20/20\n\n\ni Fold07: preprocessor 1/1, model 20/20 (extracts)\n\n\ni Fold07: preprocessor 1/1, model 20/20 (predictions)\n\n\ni Fold08: preprocessor 1/1\n\n\n✓ Fold08: preprocessor 1/1\n\n\ni Fold08: preprocessor 1/1, model 1/20\n\n\n✓ Fold08: preprocessor 1/1, model 1/20\n\n\ni Fold08: preprocessor 1/1, model 1/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 1/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 2/20\n\n\n✓ Fold08: preprocessor 1/1, model 2/20\n\n\ni Fold08: preprocessor 1/1, model 2/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 2/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 3/20\n\n\n✓ Fold08: preprocessor 1/1, model 3/20\n\n\ni Fold08: preprocessor 1/1, model 3/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 3/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 4/20\n\n\n✓ Fold08: preprocessor 1/1, model 4/20\n\n\ni Fold08: preprocessor 1/1, model 4/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 4/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 5/20\n\n\n✓ Fold08: preprocessor 1/1, model 5/20\n\n\ni Fold08: preprocessor 1/1, model 5/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 5/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 6/20\n\n\n✓ Fold08: preprocessor 1/1, model 6/20\n\n\ni Fold08: preprocessor 1/1, model 6/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 6/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 7/20\n\n\n✓ Fold08: preprocessor 1/1, model 7/20\n\n\ni Fold08: preprocessor 1/1, model 7/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 7/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 8/20\n\n\n✓ Fold08: preprocessor 1/1, model 8/20\n\n\ni Fold08: preprocessor 1/1, model 8/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 8/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 9/20\n\n\n✓ Fold08: preprocessor 1/1, model 9/20\n\n\ni Fold08: preprocessor 1/1, model 9/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 9/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 10/20\n\n\n✓ Fold08: preprocessor 1/1, model 10/20\n\n\ni Fold08: preprocessor 1/1, model 10/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 10/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 11/20\n\n\n✓ Fold08: preprocessor 1/1, model 11/20\n\n\ni Fold08: preprocessor 1/1, model 11/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 11/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 12/20\n\n\n✓ Fold08: preprocessor 1/1, model 12/20\n\n\ni Fold08: preprocessor 1/1, model 12/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 12/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 13/20\n\n\n✓ Fold08: preprocessor 1/1, model 13/20\n\n\ni Fold08: preprocessor 1/1, model 13/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 13/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 14/20\n\n\n✓ Fold08: preprocessor 1/1, model 14/20\n\n\ni Fold08: preprocessor 1/1, model 14/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 14/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 15/20\n\n\n✓ Fold08: preprocessor 1/1, model 15/20\n\n\ni Fold08: preprocessor 1/1, model 15/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 15/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 16/20\n\n\n✓ Fold08: preprocessor 1/1, model 16/20\n\n\ni Fold08: preprocessor 1/1, model 16/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 16/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 17/20\n\n\n✓ Fold08: preprocessor 1/1, model 17/20\n\n\ni Fold08: preprocessor 1/1, model 17/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 17/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 18/20\n\n\n✓ Fold08: preprocessor 1/1, model 18/20\n\n\ni Fold08: preprocessor 1/1, model 18/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 18/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 19/20\n\n\n✓ Fold08: preprocessor 1/1, model 19/20\n\n\ni Fold08: preprocessor 1/1, model 19/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 19/20 (predictions)\n\n\ni Fold08: preprocessor 1/1, model 20/20\n\n\n✓ Fold08: preprocessor 1/1, model 20/20\n\n\ni Fold08: preprocessor 1/1, model 20/20 (extracts)\n\n\ni Fold08: preprocessor 1/1, model 20/20 (predictions)\n\n\ni Fold09: preprocessor 1/1\n\n\n✓ Fold09: preprocessor 1/1\n\n\ni Fold09: preprocessor 1/1, model 1/20\n\n\n✓ Fold09: preprocessor 1/1, model 1/20\n\n\ni Fold09: preprocessor 1/1, model 1/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 1/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 2/20\n\n\n✓ Fold09: preprocessor 1/1, model 2/20\n\n\ni Fold09: preprocessor 1/1, model 2/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 2/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 3/20\n\n\n✓ Fold09: preprocessor 1/1, model 3/20\n\n\ni Fold09: preprocessor 1/1, model 3/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 3/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 4/20\n\n\n✓ Fold09: preprocessor 1/1, model 4/20\n\n\ni Fold09: preprocessor 1/1, model 4/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 4/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 5/20\n\n\n✓ Fold09: preprocessor 1/1, model 5/20\n\n\ni Fold09: preprocessor 1/1, model 5/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 5/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 6/20\n\n\n✓ Fold09: preprocessor 1/1, model 6/20\n\n\ni Fold09: preprocessor 1/1, model 6/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 6/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 7/20\n\n\n✓ Fold09: preprocessor 1/1, model 7/20\n\n\ni Fold09: preprocessor 1/1, model 7/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 7/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 8/20\n\n\n✓ Fold09: preprocessor 1/1, model 8/20\n\n\ni Fold09: preprocessor 1/1, model 8/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 8/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 9/20\n\n\n✓ Fold09: preprocessor 1/1, model 9/20\n\n\ni Fold09: preprocessor 1/1, model 9/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 9/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 10/20\n\n\n✓ Fold09: preprocessor 1/1, model 10/20\n\n\ni Fold09: preprocessor 1/1, model 10/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 10/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 11/20\n\n\n✓ Fold09: preprocessor 1/1, model 11/20\n\n\ni Fold09: preprocessor 1/1, model 11/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 11/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 12/20\n\n\n✓ Fold09: preprocessor 1/1, model 12/20\n\n\ni Fold09: preprocessor 1/1, model 12/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 12/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 13/20\n\n\n✓ Fold09: preprocessor 1/1, model 13/20\n\n\ni Fold09: preprocessor 1/1, model 13/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 13/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 14/20\n\n\n✓ Fold09: preprocessor 1/1, model 14/20\n\n\ni Fold09: preprocessor 1/1, model 14/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 14/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 15/20\n\n\n✓ Fold09: preprocessor 1/1, model 15/20\n\n\ni Fold09: preprocessor 1/1, model 15/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 15/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 16/20\n\n\n✓ Fold09: preprocessor 1/1, model 16/20\n\n\ni Fold09: preprocessor 1/1, model 16/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 16/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 17/20\n\n\n✓ Fold09: preprocessor 1/1, model 17/20\n\n\ni Fold09: preprocessor 1/1, model 17/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 17/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 18/20\n\n\n✓ Fold09: preprocessor 1/1, model 18/20\n\n\ni Fold09: preprocessor 1/1, model 18/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 18/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 19/20\n\n\n✓ Fold09: preprocessor 1/1, model 19/20\n\n\ni Fold09: preprocessor 1/1, model 19/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 19/20 (predictions)\n\n\ni Fold09: preprocessor 1/1, model 20/20\n\n\n✓ Fold09: preprocessor 1/1, model 20/20\n\n\ni Fold09: preprocessor 1/1, model 20/20 (extracts)\n\n\ni Fold09: preprocessor 1/1, model 20/20 (predictions)\n\n\ni Fold10: preprocessor 1/1\n\n\n✓ Fold10: preprocessor 1/1\n\n\ni Fold10: preprocessor 1/1, model 1/20\n\n\n✓ Fold10: preprocessor 1/1, model 1/20\n\n\ni Fold10: preprocessor 1/1, model 1/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 1/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 2/20\n\n\n✓ Fold10: preprocessor 1/1, model 2/20\n\n\ni Fold10: preprocessor 1/1, model 2/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 2/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 3/20\n\n\n✓ Fold10: preprocessor 1/1, model 3/20\n\n\ni Fold10: preprocessor 1/1, model 3/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 3/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 4/20\n\n\n✓ Fold10: preprocessor 1/1, model 4/20\n\n\ni Fold10: preprocessor 1/1, model 4/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 4/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 5/20\n\n\n✓ Fold10: preprocessor 1/1, model 5/20\n\n\ni Fold10: preprocessor 1/1, model 5/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 5/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 6/20\n\n\n✓ Fold10: preprocessor 1/1, model 6/20\n\n\ni Fold10: preprocessor 1/1, model 6/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 6/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 7/20\n\n\n✓ Fold10: preprocessor 1/1, model 7/20\n\n\ni Fold10: preprocessor 1/1, model 7/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 7/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 8/20\n\n\n✓ Fold10: preprocessor 1/1, model 8/20\n\n\ni Fold10: preprocessor 1/1, model 8/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 8/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 9/20\n\n\n✓ Fold10: preprocessor 1/1, model 9/20\n\n\ni Fold10: preprocessor 1/1, model 9/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 9/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 10/20\n\n\n✓ Fold10: preprocessor 1/1, model 10/20\n\n\ni Fold10: preprocessor 1/1, model 10/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 10/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 11/20\n\n\n✓ Fold10: preprocessor 1/1, model 11/20\n\n\ni Fold10: preprocessor 1/1, model 11/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 11/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 12/20\n\n\n✓ Fold10: preprocessor 1/1, model 12/20\n\n\ni Fold10: preprocessor 1/1, model 12/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 12/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 13/20\n\n\n✓ Fold10: preprocessor 1/1, model 13/20\n\n\ni Fold10: preprocessor 1/1, model 13/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 13/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 14/20\n\n\n✓ Fold10: preprocessor 1/1, model 14/20\n\n\ni Fold10: preprocessor 1/1, model 14/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 14/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 15/20\n\n\n✓ Fold10: preprocessor 1/1, model 15/20\n\n\ni Fold10: preprocessor 1/1, model 15/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 15/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 16/20\n\n\n✓ Fold10: preprocessor 1/1, model 16/20\n\n\ni Fold10: preprocessor 1/1, model 16/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 16/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 17/20\n\n\n✓ Fold10: preprocessor 1/1, model 17/20\n\n\ni Fold10: preprocessor 1/1, model 17/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 17/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 18/20\n\n\n✓ Fold10: preprocessor 1/1, model 18/20\n\n\ni Fold10: preprocessor 1/1, model 18/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 18/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 19/20\n\n\n✓ Fold10: preprocessor 1/1, model 19/20\n\n\ni Fold10: preprocessor 1/1, model 19/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 19/20 (predictions)\n\n\ni Fold10: preprocessor 1/1, model 20/20\n\n\n✓ Fold10: preprocessor 1/1, model 20/20\n\n\ni Fold10: preprocessor 1/1, model 20/20 (extracts)\n\n\ni Fold10: preprocessor 1/1, model 20/20 (predictions)"
  },
  {
    "objectID": "hyperparameter-tuning.html#autoplotting",
    "href": "hyperparameter-tuning.html#autoplotting",
    "title": "Machine Learning Pipline for Regression",
    "section": "Autoplotting",
    "text": "Autoplotting\n\nautoplot_results &lt;- autoplot(model_params) +\n  labs(title = \"Random Forest Hyperparameter Tuning Results\",\n       subtitle = \"Performance across parameter combinations\") +\n  theme_minimal()\n\nprint(autoplot_results)\n\n\n\n\n\n\n\n\nMean average error does not show a clear relationship with randomly selected predictors, number of trees, or minimal node size. Root mean squared error increases with randomly selected predictors, but there is no effect of rmse of more trees or of an increase in minimal node size. R-squared decreases as randomly selected predictors increase, but the effect on rsq is not clear with increasing tree size or node size. Main takeaway is the fewer randomly selected trees, the better, but the other parameters don’t matter as much. - As the number of randomly selected predictors increases, r squared decreasees and the error increases. Thus, it’s better to have fewer randomly selected predictors for accuracy. Mean average error stays about the same except for an outlier at the very lowest amount of predictors. - As the number of trees increases, there is no change in rmse or rsq. At the lowest and highest ends, it looks like the number of trees is random. Mean average error doesn’t change with this number of trees either. - Minimal node size stays has no effect on mae, appears to have a weak negative correlation with rmse, and no correlation with rsq.\n\nCollecting Metrics\n\nmodel_params %&gt;% \n  collect_metrics() %&gt;% \n  filter(.metric == \"rsq\") %&gt;% \n  slice_max(mean, n = 5)\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     3   592    14 rsq     standard   0.923    10  0.0129 Preprocessor1_Model12\n2     2   376     6 rsq     standard   0.923    10  0.0130 Preprocessor1_Model11\n3     2   560     5 rsq     standard   0.923    10  0.0133 Preprocessor1_Model07\n4     3   792    15 rsq     standard   0.922    10  0.0127 Preprocessor1_Model20\n5     3   396    12 rsq     standard   0.922    10  0.0132 Preprocessor1_Model13\n\n\nShowing the Best Outcome\n\n# Show best performing combinations\nshow_best(model_params, metric = \"rmse\", n = 5)\n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     3   592    14 rmse    standard   0.416    10  0.0437 Preprocessor1_Model12\n2     3   396    12 rmse    standard   0.418    10  0.0443 Preprocessor1_Model13\n3     3   792    15 rmse    standard   0.418    10  0.0436 Preprocessor1_Model20\n4     4   455     9 rmse    standard   0.419    10  0.0448 Preprocessor1_Model14\n5     2   560     5 rmse    standard   0.419    10  0.0443 Preprocessor1_Model07\n\nshow_best(model_params, metric = \"rsq\", n = 5)  \n\n# A tibble: 5 × 9\n   mtry trees min_n .metric .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1     3   592    14 rsq     standard   0.923    10  0.0129 Preprocessor1_Model12\n2     2   376     6 rsq     standard   0.923    10  0.0130 Preprocessor1_Model11\n3     2   560     5 rsq     standard   0.923    10  0.0133 Preprocessor1_Model07\n4     3   792    15 rsq     standard   0.922    10  0.0127 Preprocessor1_Model20\n5     3   396    12 rsq     standard   0.922    10  0.0132 Preprocessor1_Model13\n\n\nJust like the autoplot showed, a lower mtry has a higher rsq and lower rmse. The min_n is pretty random, as is the number of trees, but the top value of mtry = 3, trees = 592, and min_n = 14 is the same for metric = rmse and metric = rsq.\n\n\nBest Parameters\n\nhp_best &lt;- select_best(model_params)\n\nWarning in select_best(model_params): No value of `metric` was given; \"rmse\"\nwill be used.\n\n\n\n\nFinalizing Workflow\n\nfinal_wf &lt;- finalize_workflow(rf_wf3_tune, hp_best)\n\n\n\nFitting Finalized Workflow to Data Split\n\nlastft &lt;- last_fit(final_wf, camels_split3)\n\nChecking Performance of last model\n\ncollect_metrics(lastft)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard       0.381 Preprocessor1_Model1\n2 rsq     standard       0.951 Preprocessor1_Model1\n\n\nThe performance is .03 higher in rsq and .04 lower in rmse, so better overall than the training data alone. At an rsq of .95, this means 95 percent of the variability can be explained by the model. According to the RMSE, predictions are off by .38 units on average, whcih could be bad if the data aren’t fractional, since a high rmse makes models less accurate.\n\n\nCollecting Predictions\n\npredictions &lt;- collect_predictions(lastft)\n\n\n\nInterpret these results. How does the final model perform on the test data? Is it better or worse than the training data? Use your knowledge of the regression based metrics to describe the results.\nThis is actually really good because we have a lower rmse than with the testing data alone and the rsq is .959, better than when we collected metrics on the tuned model. .959 r-squared means 95.9 percent of the variability is explained by the model. It captures almost all of the variability influencing streamflow. there might be some overfitting, which could be verified by giving the model more data."
  },
  {
    "objectID": "hyperparameter-tuning.html#ggplotting-predicted-vs-actual-values",
    "href": "hyperparameter-tuning.html#ggplotting-predicted-vs-actual-values",
    "title": "Machine Learning Pipline for Regression",
    "section": "Ggplotting Predicted vs actual values",
    "text": "Ggplotting Predicted vs actual values\n\npredictions %&gt;% \nggplot(aes(x = .pred, y = q_mean)) +\n          geom_smooth(method = \"lm\")+\n          geom_abline() +\n          labs(title = \"Actual vs Predicted Values for A Random Forest Streamflow Model\",\n               subtitle = \"CAMELS Dataset\",\n               x = \"Predicted\",\n               y = \"Actual\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nfinalft &lt;- fit(final_wf, camels)\n\nfinal_augment &lt;- augment(finalft, camels) %&gt;% \n  select(.pred, q_mean, gauge_lon, gauge_lat) %&gt;% \n  mutate(res = (.pred - q_mean)^2)\n\nNew names:\n• `...1` -&gt; `...2`"
  },
  {
    "objectID": "hyperparameter-tuning.html#mapping-our-results",
    "href": "hyperparameter-tuning.html#mapping-our-results",
    "title": "Machine Learning Pipline for Regression",
    "section": "Mapping Our Results",
    "text": "Mapping Our Results\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\n# Custom theme to center titles\ncentered_theme &lt;- function() {\n  ggthemes::theme_map() +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 12, face = \"bold\")  # 0.5 = center\n    )\n}\n\n# Predicted Streamflow Map\npredmap &lt;- ggplot(data = final_augment, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = .pred)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\", name = \"Predicted Flow\") +\n  centered_theme() +  # Use the custom theme\n  labs(title = \"Predicted Streamflow\")\n\n# Residuals Map\nresid &lt;- ggplot(data = final_augment, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = res)) +\n  scale_color_gradient2(\n    low = \"skyblue\", mid = \"white\", high = \"maroon\", \n    midpoint = 0, name = \"Residuals\"  # Diverging scale for residuals\n  ) +\n  centered_theme() +\n  labs(title = \"Residuals\")\n\n# Combine plots\npredmap / resid +\n  plot_annotation(title = \"Model Performance Across Watersheds\", \n                 theme = theme(plot.title = element_text(hjust = 0.5, size = 14)))"
  },
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "",
    "text": "title: ‘Lab 6: Machine Learning in Hydrology’ subtitle: ‘Using Tidymodels & CAMELS Data’ date: “2025-04-10” author: name: Josh Puyear email: “puyearjosh@gmail.com” project: output-dir: docs format: html execute: echo: true\ntidymodels is an R framework designed for machine learning and statistical modeling. Built on the principles of the tidyverse, tidymodels provides a consistent and modular approach to tasks like data preprocessing, model training, evaluation, and validation. By leveraging the strengths of packages such as recipes, parsnip, and yardstick, tidymodels streamlines the modeling workflow, making it easier to experiment with different models while maintaining reproducibility and interpretability.\nloading packages for analysis\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n\n\n✔ broom        1.0.7     ✔ recipes      1.2.1\n✔ dials        1.4.0     ✔ rsample      1.3.0\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.3.0\n✔ modeldata    1.4.0     ✔ workflows    1.2.0\n✔ parsnip      1.3.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.4     ✔ yardstick    1.3.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(visdat)\nlibrary(sf)\n\nLinking to GEOS 3.13.0, GDAL 3.10.1, PROJ 9.5.1; sf_use_s2() is TRUE\n\nlibrary(parsnip)\nlibrary(recipes)\nlibrary(yardstick)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.4     ✔ stringr   1.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(ggthemes)\nWhy the hype about stream data? This is a large dataset that’s helped to enhance deep learning and model tuning while being a tool for understanding model behavior. It facilitates large-scale model comparisons and allows for hybrid models that combine physics based models with machine learning.\nWhat’s in the data? Each record in the CAMELS dataset represents a unique river basin, identified by an outlet USGS NWIS gauge_id. The dataset contains a mix of continuous and categorical variables, including meteorological, catchment, and streamflow summaries.\nThe data we are going to downloaded are the basin level summaries. For example, if we looked at row 1 of the data (Gage: 01013500) all of the values are the areal average for the drainage basin, while the flow metrics are associated with the outlet gage.\nThe CAMELS dataset is hosted by NCAR and can be accessed here under the “Individual Files” section. The root URL for all data seen on the “Individual Files” page is:\n#root  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\nNear the bottom of that page, there are many .txt files that contain the data we want. Some hold climate data for each basin, some hold geology data, some hold soil data, etc. There is also a PDF with descriptions of the columns in each file. We are going to download all of the .txt files and the PDF."
  },
  {
    "objectID": "lab6.html#getting-the-documentation-pdf",
    "href": "lab6.html#getting-the-documentation-pdf",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Getting the documentation PDF",
    "text": "Getting the documentation PDF\n\n#file &lt;- 'https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf'\n#download.file(file, \"C:/Users/Joshua Puyear/Documents/csu-undergrad/ess-330-joshp-2025/github/ess-330-labs/06-hydrology-ml/docs/camels_attributes.pdf\", mode = \"wb\")"
  },
  {
    "objectID": "lab6.html#getting-basin-characteristics",
    "href": "lab6.html#getting-basin-characteristics",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Getting Basin Characteristics",
    "text": "Getting Basin Characteristics\n\nWarning: You can no longer download the data from the website\nThe glue package provides an efficient way to interpolate and manipulate strings. It is particularly useful for dynamically constructing text, formatting outputs, and embedding R expressions within strings.\nNow we want to download the .txt files that store the actual data documented in the PDF. Doing this file by file (like we did with the PDF) is possible, but lets look at a better/easier way…\n\na. Lets create a vector storing the data types/file names we want to download:\n\n#types &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n\n\nb. Using glue, we can construct the needed URLs and file names for the data we want to download:\n\n# Where the files live online ...\n#remote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\n#local_files   &lt;- glue('data/camels_{types}.txt')\n\n\n\nc. Now we can download the data: walk2 comes from the purrr package and is used to apply a function to multiple arguments in parallel (much like map2 works over paired lists). Here, we are asking walk2 to pass the first element of remote_files and the first element of local_files to the download.file function to download the data, and setting quiet = TRUE to suppress output. The process is then iterated for the second element of each vector, and so on.\n\n#walk2(remote_files, local_files, download.file, quiet = TRUE)\n\n\n\nd. Once downloaded, the data can be read into R using readr::read_delim(), again instead of applying this to each file individually, we can use map to apply the function to each element of the local_files list.\n\n# Read and merge data\n#camels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\n\n\ne. This gives us a list of data.frames, one for each file that we want to merge into a single table. So far in class we have focused on *_join functions to merge data based on a primary and foreign key relationship.\nIn this current list, we have &gt;2 tables, but, have a shared column called gauge_id that we can use to merge the data. However, since we have more then a left and right hand table, we need a more robust tool. We will use the powerjoin package to merge the data into a single data frame. powerjoin is a flexible package for joining lists of data.frames. It provides a wide range of join types, including inner, left, right, full, semi, anti, and cross joins making it a versatile tool for data manipulation and analysis, and one that should feel familiar to users of dplyr.\nIn this case, we are join to merge every data.frame in the list (n = 6) by the shared gauge_id column. Since we want to keep all data, we want a full join.\n\n#camels &lt;- power_full_join(camels, by = 'gauge_id')\n\n\n\n\nTAs: The data are no longer accessible but luckily I had already downloaded and power_full_joined the data\n\ncamels &lt;- read_csv(\"C:/Users/Joshua Puyear/Documents/csu-undergrad/ess-330-joshp-2025/github/ess-330-labs/06-hydrology-ml/data/camels.csv\")\n\nNew names:\nRows: 671 Columns: 60\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(6): gauge_id, high_prec_timing, low_prec_timing, geol_1st_class, geol_... dbl\n(54): ...1, p_mean, pet_mean, p_seasonality, frac_snow, aridity, high_pr...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`"
  },
  {
    "objectID": "lab6.html#exploratory-data-analysis",
    "href": "lab6.html#exploratory-data-analysis",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nFirst, lets make a map of the sites. Use the borders() ggplot function to add state boundaries to the map and initially color the points by the mean flow (q_mean) at each site.\n\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map() +\n  labs(\n    title = \"Mean Streamflow Across the US\",\n    color = \"Mean Streamflow q\"\n  )\n\n\n\n\n\n\n\n\n\nQ1 Answers\nAt this point, all of the data and the PDF are downloaded into my directory\nzero_q_freq represents frequency of days with Q = 0 mm/day, and is listed as a percentage."
  },
  {
    "objectID": "lab6.html#model-preparation",
    "href": "lab6.html#model-preparation",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Model Preparation",
    "text": "Model Preparation\nAs an initial analysis, lets look at the relationship between aridity, rainfall and mean flow. First, lets make sure there is not significant correlation between these variables. Here, we make sure to drop NAs and only view the 3 columns of interest.\n\n#making a correlation matrix\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\nEven though aridity has a strong inverse correlation with streamflow and prcip has a strong positive correlation with streamflow, we’re still using these variables to predict the model"
  },
  {
    "objectID": "lab6.html#visual-eda",
    "href": "lab6.html#visual-eda",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Visual EDA",
    "text": "Visual EDA\n\na. Lets start by looking that the 3 dimensions (variables) of this data. We’ll start with a XY plot of aridity and rainfall. We are going to use the scale_color_viridis_c() function to color the points by the q_mean column. This scale functions maps the color of the points to the values in the q_mean column along the viridis continuous (c) palette. Because a scale_color_* function is applied, it maps to the known color aesthetic in the plot.\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nBy showing an x-y axis and color, it is possible for two variables to predict a third. These three dimensions can explain what’s happening with streamflow. At this point, we’re just showing what’s happening in the available data.\nSo it looks like there is a relationship between precipitation, aridity, and rainfall but it looks like an exponential decay function and is certainly not linear.\nTo test a transformation, we can log transform the x and y axes using the scale_x_log10() and scale_y_log10() functions:\n\n#when you plot the logged version of this, the relationship becomes linear because of how logs turn multiplicative properties into additive ones.\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nGreat! We can see a log-log relationship between aridity and rainfall provides a more linear relationship. This is a common relationship in hydrology and is often used to estimate rainfall in ungauged basins. However, once the data are transformed, the lack of spread in the streamflow data is quite evident with high mean flow values being compressed to the low end of aridity/high end of rainfall.\nTo address this, we can visualize how a log transform may benifit the q_mean data as well. Since the data is represented by color, rather then an axis, we can use the trans (transform) argument in the scale_color_viridis_c() function to log transform the color scale.\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nExcellent! Treating these three right skewed variables as log transformed, we can see a more evenly spread relationship between aridity, rainfall, and mean flow. This is a good sign for building a model to predict mean flow using aridity and rainfall."
  },
  {
    "objectID": "lab6.html#naive-base-lm-approach",
    "href": "lab6.html#naive-base-lm-approach",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Naive base lm approach",
    "text": "Naive base lm approach\nOk, to start, lets do what we are comfortable with … fitting a linear model to the data. First, we use prep and bake on the training data to apply the recipe. Then, we fit a linear model to the data.\n\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "lab6.html#where-things-get-a-little-messy",
    "href": "lab6.html#where-things-get-a-little-messy",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Where things get a little messy…",
    "text": "Where things get a little messy…\nOk so now we have our trained model lm_base and want to validate it on the test data.\nRemember a models ability to predict on new data is the most important part of the modeling process. It really doesnt matter how well it does on data it has already seen!\nWe have to be careful about how we do this with the base R approach:\nDon’t use augment directly on the test data before preprocessing has been applied to the data, otherwise the values it will add to the data will be incorrect Don’t use predict on the test data before preprocessing steps have been applied to the test data"
  },
  {
    "objectID": "lab6.html#correct-version-prep---bake---predict",
    "href": "lab6.html#correct-version-prep---bake---predict",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Correct version: prep -> bake -> predict",
    "text": "Correct version: prep -&gt; bake -&gt; predict\nTo correctly evaluate the model on the test data, we need to apply the same preprocessing steps to the test data that we applied to the training data. We can do this using the prep and bake functions with the recipe object. This ensures the test data is transformed in the same way as the training data before making predictions.\n\ntest_data &lt;- bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)"
  },
  {
    "objectID": "lab6.html#model-evaluation-statistical-and-visual",
    "href": "lab6.html#model-evaluation-statistical-and-visual",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Model Evaluation: statistical and visual",
    "text": "Model Evaluation: statistical and visual\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")"
  },
  {
    "objectID": "lab6.html#using-a-workflow-instead",
    "href": "lab6.html#using-a-workflow-instead",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Using a workflow instead",
    "text": "Using a workflow instead\ntidymodels provides a framework for building and evaluating models using a consistent and modular workflow. The workflows package allows you to define a series of modeling steps, including data preprocessing, model fitting, and model fitting, in a single object. This makes it easier to experiment with different models, compare performance, and ensure reproducibility.\nworkflows are built from a model, a preprocessor, and a execution. Here, we are going to use the linear_reg function to define a linear regression model, set the engine to lm, and the mode to regression. We then add our recipe to the workflow, fit the model to the training data, and extract the model coefficients.\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train)\n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\nLets ensure we replicated the results from the lm_base model. How do they look to you?\n\n# From the base implementation\nsummary(lm_base)$coefficients\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01"
  },
  {
    "objectID": "lab6.html#making-predictions",
    "href": "lab6.html#making-predictions",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Making Predictions",
    "text": "Making Predictions\nNow that lm_wf is a workflow, data is not embedded in the model, we can use augment with the new_data argument to make predictions on the test data.\n\n#\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\n\nNew names:\n• `...1` -&gt; `...3`\n\ndim(lm_data)\n\n[1] 135  62"
  },
  {
    "objectID": "lab6.html#model-evaluation-statistical-and-visual-1",
    "href": "lab6.html#model-evaluation-statistical-and-visual-1",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Model Evaluation: Statistical and Visual",
    "text": "Model Evaluation: Statistical and Visual\nAs with EDA, applying for graphical and statistical evaluation of the model is a key Here, we use the metrics function to extract the default metrics (rmse, rsq, mae) between the observed and predicted mean streamflow values.\nWe then create a scatter plot of the observed vs predicted values, colored by aridity, to visualize the model performance.\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()"
  },
  {
    "objectID": "lab6.html#switch-it-up",
    "href": "lab6.html#switch-it-up",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Switch it up!",
    "text": "Switch it up!\nThe real power of this approach is that we can easily switch out the models/recipes and see how it performs. Here, we are going to instead use a random forest model to predict mean streamflow. We define a random forest model using the rand_forest function, set the engine to ranger, and the mode to regression. We then add the recipe, fit the model, and evaluate the skill.\n\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train)"
  },
  {
    "objectID": "lab6.html#predictions",
    "href": "lab6.html#predictions",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Predictions",
    "text": "Predictions\nMake predictions on the test data using the augment function and the new_data argument.\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\n\nNew names:\n• `...1` -&gt; `...2`\n\ndim(rf_data)\n\n[1] 135  61"
  },
  {
    "objectID": "lab6.html#model-evaluation-statistical-and-visual-2",
    "href": "lab6.html#model-evaluation-statistical-and-visual-2",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Model Evaluation: statistical and visual",
    "text": "Model Evaluation: statistical and visual\nEvaluate the model using the metrics function and create a scatter plot of the observed vs predicted values, colored by aridity.\n\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.596\n2 rsq     standard       0.733\n3 mae     standard       0.370\n\n\n\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\nAwesome! We just set up a completely new model and were able to utilize all of the things we had done for the linear model. This is the power of the tidymodels framework!\nThat said, we still can reduce some to the repetition. Further, we are not really able to compare these models to one another as they"
  },
  {
    "objectID": "lab6.html#a-workflowset-approach",
    "href": "lab6.html#a-workflowset-approach",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "A workflowset approach",
    "text": "A workflowset approach\nworkflow_set is a powerful tool for comparing multiple models on the same data. It allows you to define a set of workflows, fit them to the same data, and evaluate their performance using a common metric. Here, we are going to create a workflow_set object with the linear regression and random forest models, fit them to the training data, and compare their performance using the autoplot and rank_results functions.\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.565  0.0243    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.770  0.0255    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n\n\nOverall it seems the random forest model is outperforming the linear model. This is not surprising given the non-linear relationship between the predictors and the outcome :)"
  },
  {
    "objectID": "lab6.html#data-splitting-15",
    "href": "lab6.html#data-splitting-15",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Data Splitting (15)",
    "text": "Data Splitting (15)\n\nSet a seed for reproducible\n\nset.seed(295)\n\n\n\nCreate an initial split with 75% used for training and 25% for testing\n\ncamels_split2 &lt;- initial_split(camels, prop = 0.75)\n\n\n\nExtract your training and testing sets. Build a 10-fold CV dataset as well\n\ncamels_train2 &lt;- training(camels_split2)\ncamels_test2  &lt;- testing(camels_split2)\n\n#ten-fold cross-validation dataset\ncamels_cv2 &lt;- vfold_cv(camels_train2, v = 10)"
  },
  {
    "objectID": "lab6.html#recipe-15",
    "href": "lab6.html#recipe-15",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Recipe (15)",
    "text": "Recipe (15)\n\nDefine a formula you want to use to predict logQmean\n\n\nBuild a recipe that you feel handles the predictors chosen well\n\n#here i was wondering if it would be worthwhile to try to plot all of these variables, or if it's better to make them a part of the model and just see if it works.\n\nrec2 &lt;-  recipe(logQmean ~ aridity + p_mean + high_prec_freq + low_prec_freq + high_prec_dur + lai_max + water_frac + slope_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  #step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\nstep_interact(terms = ~ aridity:p_mean + aridity:lai_max + p_mean:low_prec_freq + high_prec_freq:low_prec_freq + low_prec_freq:aridity) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n\n\nDescribe in words why you are choosing the formula you are. Consult the downloaded PDF for the data to help you make this decision.\n\nI chose high_prec_freq (high precipitation frequency) because it’s distant enough from p_mean to be a different concept.\nlow_prec_freq will reflect droughts frequency, which will be directly related to average streamflow for at least part of the season\nhigh_prec_dur will greatly weigh into average sreamflow because it describes how often events 9x the median flow occur.\nlai_max is an ecological predictor that assumes in areas with broad leaves there is more available moisture in the environment, which should increase streamflow\nfraction of the top 1.5m of soil marked as water- this should be positively correlated with streamflow if there is an overall higher water table *slope_mean should contribute to streamflow because a higher slope will increase velocity\n\nSince there are so many terms, I am not going to try to visualize all of them. I will run the model and see if it works. I am curious about which terms interact though, so I think I’ll make a correlation table.\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean, high_prec_freq, low_prec_freq, high_prec_dur, lai_max, soil_conductivity, water_frac) |&gt; \n  drop_na() |&gt; \n  cor()\n\n                      aridity      p_mean      q_mean high_prec_freq\naridity            1.00000000 -0.75500903 -0.58177710     0.50924192\np_mean            -0.75500903  1.00000000  0.88657569    -0.60030682\nq_mean            -0.58177710  0.88657569  1.00000000    -0.66875889\nhigh_prec_freq     0.50924192 -0.60030682 -0.66875889     1.00000000\nlow_prec_freq      0.74178481 -0.72523441 -0.71457114     0.87176299\nhigh_prec_dur      0.41619443 -0.10924664  0.09566366     0.16493933\nlai_max           -0.70476393  0.58782662  0.37907296    -0.36234645\nsoil_conductivity -0.03011819  0.08253143  0.03534482    -0.10870127\nwater_frac         0.03283288 -0.06632260 -0.03646015    -0.03122678\n                  low_prec_freq high_prec_dur     lai_max soil_conductivity\naridity              0.74178481   0.416194434 -0.70476393       -0.03011819\np_mean              -0.72523441  -0.109246644  0.58782662        0.08253143\nq_mean              -0.71457114   0.095663663  0.37907296        0.03534482\nhigh_prec_freq       0.87176299   0.164939334 -0.36234645       -0.10870127\nlow_prec_freq        1.00000000   0.349279796 -0.59053772       -0.07317975\nhigh_prec_dur        0.34927980   1.000000000 -0.47842289        0.05751086\nlai_max             -0.59053772  -0.478422889  1.00000000        0.02222807\nsoil_conductivity   -0.07317975   0.057510862  0.02222807        1.00000000\nwater_frac          -0.01544770  -0.006140105 -0.04983244        0.07338496\n                    water_frac\naridity            0.032832877\np_mean            -0.066322596\nq_mean            -0.036460152\nhigh_prec_freq    -0.031226777\nlow_prec_freq     -0.015447702\nhigh_prec_dur     -0.006140105\nlai_max           -0.049832438\nsoil_conductivity  0.073384956\nwater_frac         1.000000000\n\n\n\n\nDefine 3 models (25)\n\n#random forest with ranger and regression\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n#I will be using the neural network model and xgboost\n\n\n\nworkflow set ()\n\nwf2 &lt;- workflow_set(list(rec2), list(lm_model, rf_model, nnet_model, xgboost_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv2) \n\nautoplot(wf2)\n\n\n\n\n\n\n\nrank_results(wf2, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.379  0.0369    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.896  0.0170    10 recipe       rand…     1\n3 recipe_boost_tree Prepro… rmse    0.369  0.0382    10 recipe       boos…     2\n4 recipe_boost_tree Prepro… rsq     0.891  0.0244    10 recipe       boos…     2\n5 recipe_linear_reg Prepro… rmse    0.459  0.0385    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.852  0.0187    10 recipe       line…     3\n7 recipe_bag_mlp    Prepro… rmse    0.741  0.0627    10 recipe       bag_…     4\n8 recipe_bag_mlp    Prepro… rsq     0.804  0.0360    10 recipe       bag_…     4\n\n\nThe random forest did the best with an r squared of.89, followed by rand_forest at .88, according to the autoplot. I think these tree-based models did better because there wasn’t a lot of data to go by, allowing for more accurate modeling with decision trees. Boost_tree builds sequentially, while random forests build parallel. The sequential building was slightly better."
  },
  {
    "objectID": "lab6.html#extact-and-evaluate",
    "href": "lab6.html#extact-and-evaluate",
    "title": "Lab 6: Machine Learning in Hydrology",
    "section": "Extact and Evaluate",
    "text": "Extact and Evaluate\nNow that you found your favorite model, lets see how it does on the test data!\n\nBuild a workflow (not workflow set) with your favorite model, recipe, and training data\n\n\nUse fit to fit all training data to the model\n\nrandflow &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec2) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train2)\n\n\n\nUse augment to make predictions on the test data\n\nrandflow_data &lt;- augment(randflow, new_data = camels_test2)\n\nNew names:\n• `...1` -&gt; `...2`\n\ndim(randflow_data)\n\n[1] 168  61\n\n\n\n\nCreate a plot of the observed vs predicted values with clear title, axis labels, and a compelling color scale\n\nggplot(randflow_data, aes(x = logQmean, y = .pred)) +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw() +\n  labs(\n    y = \"Predictor variables\",\n    x = \"Streamflow\",\n    title = \"Boosted Forest Prediction model\"\n  )\n\n\n\n\n\n\n\n\n\n\nDescribe what you think of the results!\nThe line of best fit is much more accurate at higher log streamflow, but at lower streamflow the values are more spread out. It appears as though since the R^2 converges, there might be more interacting variables I wasn’t catching at first."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lab 6: Machine Learning Steps",
    "section": "",
    "text": "tidymodels is an R framework designed for machine learning and statistical modeling. Built on the principles of the tidyverse, tidymodels provides a consistent and modular approach to tasks like data preprocessing, model training, evaluation, and validation. By leveraging the strengths of packages such as recipes, parsnip, and yardstick, tidymodels streamlines the modeling workflow, making it easier to experiment with different models while maintaining reproducibility and interpretability.\nloading packages for analysis\nCode\nlibrary(tidymodels)\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n\n\n✔ broom        1.0.7     ✔ recipes      1.2.1\n✔ dials        1.4.0     ✔ rsample      1.3.0\n✔ dplyr        1.1.4     ✔ tibble       3.2.1\n✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n✔ infer        1.0.7     ✔ tune         1.3.0\n✔ modeldata    1.4.0     ✔ workflows    1.2.0\n✔ parsnip      1.3.1     ✔ workflowsets 1.1.0\n✔ purrr        1.0.4     ✔ yardstick    1.3.2\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(visdat)\nlibrary(sf)\n\n\nLinking to GEOS 3.13.0, GDAL 3.10.1, PROJ 9.5.1; sf_use_s2() is TRUE\n\n\nCode\nlibrary(parsnip)\nlibrary(recipes)\nlibrary(yardstick)\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ lubridate 1.9.4     ✔ stringr   1.5.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ readr::col_factor() masks scales::col_factor()\n✖ purrr::discard()    masks scales::discard()\n✖ dplyr::filter()     masks stats::filter()\n✖ stringr::fixed()    masks recipes::fixed()\n✖ dplyr::lag()        masks stats::lag()\n✖ readr::spec()       masks yardstick::spec()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\n\nCode\nlibrary(baguette)\nlibrary(ggthemes)\nWhy the hype about stream data? This is a large dataset that’s helped to enhance deep learning and model tuning while being a tool for understanding model behavior. It facilitates large-scale model comparisons and allows for hybrid models that combine physics based models with machine learning.\nWhat’s in the data? Each record in the CAMELS dataset represents a unique river basin, identified by an outlet USGS NWIS gauge_id. The dataset contains a mix of continuous and categorical variables, including meteorological, catchment, and streamflow summaries.\nThe data we are going to downloaded are the basin level summaries. For example, if we looked at row 1 of the data (Gage: 01013500) all of the values are the areal average for the drainage basin, while the flow metrics are associated with the outlet gage.\nThe CAMELS dataset is hosted by NCAR and can be accessed here under the “Individual Files” section. The root URL for all data seen on the “Individual Files” page is:\nCode\n#root  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\nNear the bottom of that page, there are many .txt files that contain the data we want. Some hold climate data for each basin, some hold geology data, some hold soil data, etc. There is also a PDF with descriptions of the columns in each file. We are going to download all of the .txt files and the PDF."
  },
  {
    "objectID": "index.html#getting-the-documentation-pdf",
    "href": "index.html#getting-the-documentation-pdf",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Getting the documentation PDF",
    "text": "Getting the documentation PDF\n\n\nCode\n#file &lt;- 'https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf'\n#download.file(file, \"C:/Users/Joshua Puyear/Documents/csu-undergrad/ess-330-joshp-2025/github/ess-330-labs/06-hydrology-ml/docs/camels_attributes.pdf\", mode = \"wb\")"
  },
  {
    "objectID": "index.html#getting-basin-characteristics",
    "href": "index.html#getting-basin-characteristics",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Getting Basin Characteristics",
    "text": "Getting Basin Characteristics\n\nWarning: You can no longer download the data from the website\nThe glue package provides an efficient way to interpolate and manipulate strings. It is particularly useful for dynamically constructing text, formatting outputs, and embedding R expressions within strings.\nNow we want to download the .txt files that store the actual data documented in the PDF. Doing this file by file (like we did with the PDF) is possible, but lets look at a better/easier way…\n\na. Lets create a vector storing the data types/file names we want to download:\n\n\nCode\n#types &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n\n\n\nb. Using glue, we can construct the needed URLs and file names for the data we want to download:\n\n\nCode\n# Where the files live online ...\n#remote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\n#local_files   &lt;- glue('data/camels_{types}.txt')\n\n\n\n\nc. Now we can download the data: walk2 comes from the purrr package and is used to apply a function to multiple arguments in parallel (much like map2 works over paired lists). Here, we are asking walk2 to pass the first element of remote_files and the first element of local_files to the download.file function to download the data, and setting quiet = TRUE to suppress output. The process is then iterated for the second element of each vector, and so on.\n\n\nCode\n#walk2(remote_files, local_files, download.file, quiet = TRUE)\n\n\n\n\nd. Once downloaded, the data can be read into R using readr::read_delim(), again instead of applying this to each file individually, we can use map to apply the function to each element of the local_files list.\n\n\nCode\n# Read and merge data\n#camels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\n\n\n\ne. This gives us a list of data.frames, one for each file that we want to merge into a single table. So far in class we have focused on *_join functions to merge data based on a primary and foreign key relationship.\nIn this current list, we have &gt;2 tables, but, have a shared column called gauge_id that we can use to merge the data. However, since we have more then a left and right hand table, we need a more robust tool. We will use the powerjoin package to merge the data into a single data frame. powerjoin is a flexible package for joining lists of data.frames. It provides a wide range of join types, including inner, left, right, full, semi, anti, and cross joins making it a versatile tool for data manipulation and analysis, and one that should feel familiar to users of dplyr.\nIn this case, we are join to merge every data.frame in the list (n = 6) by the shared gauge_id column. Since we want to keep all data, we want a full join.\n\n\nCode\n#camels &lt;- power_full_join(camels, by = 'gauge_id')\n\n\n\n\n\nTAs: The data are no longer accessible but luckily I had already downloaded and power_full_joined the data\n\n\nCode\ncamels &lt;- read_csv(\"C:/Users/Joshua Puyear/Documents/csu-undergrad/ess-330-joshp-2025/github/ess-330-labs/06-hydrology-ml/data/camels.csv\")\n\n\nNew names:\nRows: 671 Columns: 60\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(6): gauge_id, high_prec_timing, low_prec_timing, geol_1st_class, geol_... dbl\n(54): ...1, p_mean, pet_mean, p_seasonality, frac_snow, aridity, high_pr...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`"
  },
  {
    "objectID": "index.html#exploratory-data-analysis",
    "href": "index.html#exploratory-data-analysis",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nFirst, lets make a map of the sites. Use the borders() ggplot function to add state boundaries to the map and initially color the points by the mean flow (q_mean) at each site.\n\n\nCode\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map() +\n  labs(\n    title = \"Mean Streamflow Across the US\",\n    color = \"Mean Streamflow q\"\n  )\n\n\n\n\n\n\n\n\n\n\nQ1 Answers\nAt this point, all of the data and the PDF are downloaded into my directory\nzero_q_freq represents frequency of days with Q = 0 mm/day, and is listed as a percentage."
  },
  {
    "objectID": "index.html#model-preparation",
    "href": "index.html#model-preparation",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Model Preparation",
    "text": "Model Preparation\nAs an initial analysis, lets look at the relationship between aridity, rainfall and mean flow. First, lets make sure there is not significant correlation between these variables. Here, we make sure to drop NAs and only view the 3 columns of interest.\n\n\nCode\n#making a correlation matrix\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\nEven though aridity has a strong inverse correlation with streamflow and prcip has a strong positive correlation with streamflow, we’re still using these variables to predict the model"
  },
  {
    "objectID": "index.html#visual-eda",
    "href": "index.html#visual-eda",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Visual EDA",
    "text": "Visual EDA\n\na. Lets start by looking that the 3 dimensions (variables) of this data. We’ll start with a XY plot of aridity and rainfall. We are going to use the scale_color_viridis_c() function to color the points by the q_mean column. This scale functions maps the color of the points to the values in the q_mean column along the viridis continuous (c) palette. Because a scale_color_* function is applied, it maps to the known color aesthetic in the plot.\n\n\nCode\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nBy showing an x-y axis and color, it is possible for two variables to predict a third. These three dimensions can explain what’s happening with streamflow. At this point, we’re just showing what’s happening in the available data.\nSo it looks like there is a relationship between precipitation, aridity, and rainfall but it looks like an exponential decay function and is certainly not linear.\nTo test a transformation, we can log transform the x and y axes using the scale_x_log10() and scale_y_log10() functions:\n\n\nCode\n#when you plot the logged version of this, the relationship becomes linear because of how logs turn multiplicative properties into additive ones.\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nGreat! We can see a log-log relationship between aridity and rainfall provides a more linear relationship. This is a common relationship in hydrology and is often used to estimate rainfall in ungauged basins. However, once the data are transformed, the lack of spread in the streamflow data is quite evident with high mean flow values being compressed to the low end of aridity/high end of rainfall.\nTo address this, we can visualize how a log transform may benifit the q_mean data as well. Since the data is represented by color, rather then an axis, we can use the trans (transform) argument in the scale_color_viridis_c() function to log transform the color scale.\n\n\nCode\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nExcellent! Treating these three right skewed variables as log transformed, we can see a more evenly spread relationship between aridity, rainfall, and mean flow. This is a good sign for building a model to predict mean flow using aridity and rainfall."
  },
  {
    "objectID": "index.html#naive-base-lm-approach",
    "href": "index.html#naive-base-lm-approach",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Naive base lm approach",
    "text": "Naive base lm approach\nOk, to start, lets do what we are comfortable with … fitting a linear model to the data. First, we use prep and bake on the training data to apply the recipe. Then, we fit a linear model to the data.\n\n\nCode\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "index.html#where-things-get-a-little-messy",
    "href": "index.html#where-things-get-a-little-messy",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Where things get a little messy…",
    "text": "Where things get a little messy…\nOk so now we have our trained model lm_base and want to validate it on the test data.\nRemember a models ability to predict on new data is the most important part of the modeling process. It really doesnt matter how well it does on data it has already seen!\nWe have to be careful about how we do this with the base R approach:\nDon’t use augment directly on the test data before preprocessing has been applied to the data, otherwise the values it will add to the data will be incorrect Don’t use predict on the test data before preprocessing steps have been applied to the test data"
  },
  {
    "objectID": "index.html#correct-version-prep---bake---predict",
    "href": "index.html#correct-version-prep---bake---predict",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Correct version: prep -> bake -> predict",
    "text": "Correct version: prep -&gt; bake -&gt; predict\nTo correctly evaluate the model on the test data, we need to apply the same preprocessing steps to the test data that we applied to the training data. We can do this using the prep and bake functions with the recipe object. This ensures the test data is transformed in the same way as the training data before making predictions.\n\n\nCode\ntest_data &lt;- bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)"
  },
  {
    "objectID": "index.html#model-evaluation-statistical-and-visual",
    "href": "index.html#model-evaluation-statistical-and-visual",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Model Evaluation: statistical and visual",
    "text": "Model Evaluation: statistical and visual\n\n\nCode\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\n\nCode\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")"
  },
  {
    "objectID": "index.html#using-a-workflow-instead",
    "href": "index.html#using-a-workflow-instead",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Using a workflow instead",
    "text": "Using a workflow instead\ntidymodels provides a framework for building and evaluating models using a consistent and modular workflow. The workflows package allows you to define a series of modeling steps, including data preprocessing, model fitting, and model fitting, in a single object. This makes it easier to experiment with different models, compare performance, and ensure reproducibility.\nworkflows are built from a model, a preprocessor, and a execution. Here, we are going to use the linear_reg function to define a linear regression model, set the engine to lm, and the mode to regression. We then add our recipe to the workflow, fit the model to the training data, and extract the model coefficients.\n\n\nCode\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train)\n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\nLets ensure we replicated the results from the lm_base model. How do they look to you?\n\n\nCode\n# From the base implementation\nsummary(lm_base)$coefficients\n\n\n                 Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)    -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity        -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean          1.4843771 0.15511117   9.569762 4.022500e-20\naridity:p_mean  0.1048449 0.07198145   1.456555 1.458304e-01"
  },
  {
    "objectID": "index.html#making-predictions",
    "href": "index.html#making-predictions",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Making Predictions",
    "text": "Making Predictions\nNow that lm_wf is a workflow, data is not embedded in the model, we can use augment with the new_data argument to make predictions on the test data.\n\n\nCode\n#\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\n\n\nNew names:\n• `...1` -&gt; `...3`\n\n\nCode\ndim(lm_data)\n\n\n[1] 135  62"
  },
  {
    "objectID": "index.html#model-evaluation-statistical-and-visual-1",
    "href": "index.html#model-evaluation-statistical-and-visual-1",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Model Evaluation: Statistical and Visual",
    "text": "Model Evaluation: Statistical and Visual\nAs with EDA, applying for graphical and statistical evaluation of the model is a key Here, we use the metrics function to extract the default metrics (rmse, rsq, mae) between the observed and predicted mean streamflow values.\nWe then create a scatter plot of the observed vs predicted values, colored by aridity, to visualize the model performance.\n\n\nCode\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\n\nCode\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()"
  },
  {
    "objectID": "index.html#switch-it-up",
    "href": "index.html#switch-it-up",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Switch it up!",
    "text": "Switch it up!\nThe real power of this approach is that we can easily switch out the models/recipes and see how it performs. Here, we are going to instead use a random forest model to predict mean streamflow. We define a random forest model using the rand_forest function, set the engine to ranger, and the mode to regression. We then add the recipe, fit the model, and evaluate the skill.\n\n\nCode\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train)"
  },
  {
    "objectID": "index.html#predictions",
    "href": "index.html#predictions",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Predictions",
    "text": "Predictions\nMake predictions on the test data using the augment function and the new_data argument.\n\n\nCode\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\n\n\nNew names:\n• `...1` -&gt; `...2`\n\n\nCode\ndim(rf_data)\n\n\n[1] 135  61"
  },
  {
    "objectID": "index.html#model-evaluation-statistical-and-visual-2",
    "href": "index.html#model-evaluation-statistical-and-visual-2",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Model Evaluation: statistical and visual",
    "text": "Model Evaluation: statistical and visual\nEvaluate the model using the metrics function and create a scatter plot of the observed vs predicted values, colored by aridity.\n\n\nCode\nmetrics(rf_data, truth = logQmean, estimate = .pred)\n\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.596\n2 rsq     standard       0.733\n3 mae     standard       0.370\n\n\n\n\nCode\nggplot(rf_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nAwesome! We just set up a completely new model and were able to utilize all of the things we had done for the linear model. This is the power of the tidymodels framework!\nThat said, we still can reduce some to the repetition. Further, we are not really able to compare these models to one another as they"
  },
  {
    "objectID": "index.html#a-workflowset-approach",
    "href": "index.html#a-workflowset-approach",
    "title": "Lab 6: Machine Learning Steps",
    "section": "A workflowset approach",
    "text": "A workflowset approach\nworkflow_set is a powerful tool for comparing multiple models on the same data. It allows you to define a set of workflows, fit them to the same data, and evaluate their performance using a common metric. Here, we are going to create a workflow_set object with the linear regression and random forest models, fit them to the training data, and compare their performance using the autoplot and rank_results functions.\n\n\nCode\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\n\n\nCode\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.565  0.0243    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.770  0.0255    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n\n\nOverall it seems the random forest model is outperforming the linear model. This is not surprising given the non-linear relationship between the predictors and the outcome :)"
  },
  {
    "objectID": "index.html#data-splitting-15",
    "href": "index.html#data-splitting-15",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Data Splitting (15)",
    "text": "Data Splitting (15)\n\nSet a seed for reproducible\n\n\nCode\nset.seed(295)\n\n\n\n\nCreate an initial split with 75% used for training and 25% for testing\n\n\nCode\ncamels_split2 &lt;- initial_split(camels, prop = 0.75)\n\n\n\n\nExtract your training and testing sets. Build a 10-fold CV dataset as well\n\n\nCode\ncamels_train2 &lt;- training(camels_split2)\ncamels_test2  &lt;- testing(camels_split2)\n\n#ten-fold cross-validation dataset\ncamels_cv2 &lt;- vfold_cv(camels_train2, v = 10)"
  },
  {
    "objectID": "index.html#recipe-15",
    "href": "index.html#recipe-15",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Recipe (15)",
    "text": "Recipe (15)\n\nDefine a formula you want to use to predict logQmean\n\n\nBuild a recipe that you feel handles the predictors chosen well\n\n\nCode\n#here i was wondering if it would be worthwhile to try to plot all of these variables, or if it's better to make them a part of the model and just see if it works.\n\nrec2 &lt;-  recipe(logQmean ~ aridity + p_mean + high_prec_freq + low_prec_freq + high_prec_dur + lai_max + water_frac + slope_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  #step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\nstep_interact(terms = ~ aridity:p_mean + aridity:lai_max + p_mean:low_prec_freq + high_prec_freq:low_prec_freq + low_prec_freq:aridity) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n\n\n\nDescribe in words why you are choosing the formula you are. Consult the downloaded PDF for the data to help you make this decision.\n\nI chose high_prec_freq (high precipitation frequency) because it’s distant enough from p_mean to be a different concept.\nlow_prec_freq will reflect droughts frequency, which will be directly related to average streamflow for at least part of the season\nhigh_prec_dur will greatly weigh into average sreamflow because it describes how often events 9x the median flow occur.\nlai_max is an ecological predictor that assumes in areas with broad leaves there is more available moisture in the environment, which should increase streamflow\nfraction of the top 1.5m of soil marked as water- this should be positively correlated with streamflow if there is an overall higher water table *slope_mean should contribute to streamflow because a higher slope will increase velocity\n\nSince there are so many terms, I am not going to try to visualize all of them. I will run the model and see if it works. I am curious about which terms interact though, so I think I’ll make a correlation table.\n\n\nCode\ncamels |&gt; \n  select(aridity, p_mean, q_mean, high_prec_freq, low_prec_freq, high_prec_dur, lai_max, soil_conductivity, water_frac) |&gt; \n  drop_na() |&gt; \n  cor()\n\n\n                      aridity      p_mean      q_mean high_prec_freq\naridity            1.00000000 -0.75500903 -0.58177710     0.50924192\np_mean            -0.75500903  1.00000000  0.88657569    -0.60030682\nq_mean            -0.58177710  0.88657569  1.00000000    -0.66875889\nhigh_prec_freq     0.50924192 -0.60030682 -0.66875889     1.00000000\nlow_prec_freq      0.74178481 -0.72523441 -0.71457114     0.87176299\nhigh_prec_dur      0.41619443 -0.10924664  0.09566366     0.16493933\nlai_max           -0.70476393  0.58782662  0.37907296    -0.36234645\nsoil_conductivity -0.03011819  0.08253143  0.03534482    -0.10870127\nwater_frac         0.03283288 -0.06632260 -0.03646015    -0.03122678\n                  low_prec_freq high_prec_dur     lai_max soil_conductivity\naridity              0.74178481   0.416194434 -0.70476393       -0.03011819\np_mean              -0.72523441  -0.109246644  0.58782662        0.08253143\nq_mean              -0.71457114   0.095663663  0.37907296        0.03534482\nhigh_prec_freq       0.87176299   0.164939334 -0.36234645       -0.10870127\nlow_prec_freq        1.00000000   0.349279796 -0.59053772       -0.07317975\nhigh_prec_dur        0.34927980   1.000000000 -0.47842289        0.05751086\nlai_max             -0.59053772  -0.478422889  1.00000000        0.02222807\nsoil_conductivity   -0.07317975   0.057510862  0.02222807        1.00000000\nwater_frac          -0.01544770  -0.006140105 -0.04983244        0.07338496\n                    water_frac\naridity            0.032832877\np_mean            -0.066322596\nq_mean            -0.036460152\nhigh_prec_freq    -0.031226777\nlow_prec_freq     -0.015447702\nhigh_prec_dur     -0.006140105\nlai_max           -0.049832438\nsoil_conductivity  0.073384956\nwater_frac         1.000000000\n\n\n\n\nDefine 3 models (25)\n\n\nCode\n#random forest with ranger and regression\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n#I will be using the neural network model and xgboost\n\n\n\n\nworkflow set ()\n\n\nCode\nwf2 &lt;- workflow_set(list(rec2), list(lm_model, rf_model, nnet_model, xgboost_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv2) \n\nautoplot(wf2)\n\n\n\n\n\n\n\n\n\nCode\nrank_results(wf2, rank_metric = \"rsq\", select_best = TRUE)\n\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.379  0.0369    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.896  0.0170    10 recipe       rand…     1\n3 recipe_boost_tree Prepro… rmse    0.369  0.0382    10 recipe       boos…     2\n4 recipe_boost_tree Prepro… rsq     0.891  0.0244    10 recipe       boos…     2\n5 recipe_linear_reg Prepro… rmse    0.459  0.0385    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.852  0.0187    10 recipe       line…     3\n7 recipe_bag_mlp    Prepro… rmse    0.741  0.0627    10 recipe       bag_…     4\n8 recipe_bag_mlp    Prepro… rsq     0.804  0.0360    10 recipe       bag_…     4\n\n\nThe random forest did the best with an r squared of.89, followed by rand_forest at .88, according to the autoplot. I think these tree-based models did better because there wasn’t a lot of data to go by, allowing for more accurate modeling with decision trees. Boost_tree builds sequentially, while random forests build parallel. The sequential building was slightly better."
  },
  {
    "objectID": "index.html#extact-and-evaluate",
    "href": "index.html#extact-and-evaluate",
    "title": "Lab 6: Machine Learning Steps",
    "section": "Extact and Evaluate",
    "text": "Extact and Evaluate\nNow that you found your favorite model, lets see how it does on the test data!\n\nBuild a workflow (not workflow set) with your favorite model, recipe, and training data\n\n\nUse fit to fit all training data to the model\n\n\nCode\nrandflow &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec2) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train2)\n\n\n\n\nUse augment to make predictions on the test data\n\n\nCode\nrandflow_data &lt;- augment(randflow, new_data = camels_test2)\n\n\nNew names:\n• `...1` -&gt; `...2`\n\n\nCode\ndim(randflow_data)\n\n\n[1] 168  61\n\n\n\n\nCreate a plot of the observed vs predicted values with clear title, axis labels, and a compelling color scale\n\n\nCode\nggplot(randflow_data, aes(x = logQmean, y = .pred)) +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw() +\n  labs(\n    y = \"Predictor variables\",\n    x = \"Streamflow\",\n    title = \"Boosted Forest Prediction model\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nDescribe what you think of the results!\nThe line of best fit is much more accurate at higher log streamflow, but at lower streamflow the values are more spread out. It appears as though since the R^2 converges, there might be more interacting variables I wasn’t catching at first."
  }
]
---
title: "Machine Learning Pipline for Regression"
subtitle: "CAMELS hydrological data"
date: "2025-04-18"
author: 
  name: Josh Puyear
  email: "puyearjosh@gmail.com" 
project:
 output-dir: docs
format: html
execute:
  echo: true
---

```{r, echo = TRUE}
library(tidyverse)
library(tidymodels)
```



libraries for data reading


```{r, echo = TRUE}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(visdat)
library(ggpubr)
library(skimr)
library(broom)

```



### Warning: You can no longer download the data from the website
Data download


```{r, echo = TRUE}
#root  <- 'https://gdex.ucar.edu/dataset/camels/file'
```



PDF of Data Documentation


```{r, echo = TRUE}
#file <- 'https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf'
#download.file(file, "C:/Users/Joshua Puyear/Documents/csu-undergrad/ess-330-joshp-2025/github/ess-330-labs/06-hydrology-ml/docs/camels_attributes.pdf", mode = "wb")
```



Basin Characteristics


```{r, echo = TRUE}
#types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

```



Constructing URLS with Glue


```{r, echo = TRUE}
# Where the files live online ...
#remote_files  <- glue('{root}/camels_{types}.txt')
# where we want to download the data ...
#local_files   <- glue('data/camels_{types}.txt')

```



Important: Downloading the Data with purrr walk2


```{r, echo = TRUE}
#walk2(remote_files, local_files, download.file, quiet = TRUE)
```



Finally: Mapping the data to the local_files list with map and read_delim


```{r, echo = TRUE}
# Read and merge data
#camels <- map(local_files, read_delim, show_col_types = FALSE) 
```



Performing a power_full_join to merge all tables by guage_id column


```{r, echo = TRUE}
#camels <- power_full_join(camels, by = 'gauge_id')
```

```{r, echo = TRUE}
camels <- read_csv("C:/Users/Joshua Puyear/Documents/csu-undergrad/ess-330-joshp-2025/github/ess-330-labs/06-hydrology-ml/data/camels.csv")
```



Exploratory Data Analysis: EDA Steps

## 1. Question: What are the main predictors of mean streamflow in the Poudre basin?
## 2.Data have already been read in

## 3. Summarizing the Data

### Basic Attributes with Skim


```{r, echo = TRUE}
skim(camels)
#in the last column, skim gives you a general histogram shape

```


52/58 columns are numeric and there are 671 total observations, well within the range for visdat. Skim is useful if we're concered about the size of a dataset

### Visualization


```{r, echo = TRUE}
vis_dat(camels)
```


Most of the missing values come from geol_2nd_class and root_depth_99, which we don't really care about for streamflow. Looks like a pretty complete dataset! 

### Data Summary



```{r, echo = TRUE}
summary(camels)
```



Since the median mean streamflow (q_mean) is lower than the mean streamflow, there are probably spikes of much higher streamflow interspersed with longer periods of low flow. The range of mean streamflow is between .004 mm/day (basically nothing) and 9.68mm/day. Mean precipitation p_mean is close to the median, at 3.25 and 3.22, respectively. The minimum is .6446 while the maximum is 8.94. These ranges all seem reasonable.

# cleaning the data with dplyr, etc


```{r, echo = TRUE}
library(patchwork)

gghistogram(log(camels$q_mean))

gghistogram(log(camels$p_mean))

#would log transformation normalize log p mean, which has a long tail?
#I will use step_log for any predictors that need to be logged. This will be tested with the shapiro test, which looks for normal distribution

#logging doesn't make the normality much better, and the models I'm going to choose will not be linear anyway and thus will function without normal distribution

#How can a table of shapiro test results be made?
shapiro.test(camels$q_mean)
#very much not normal data

#to see how the variables relate, find the cor test you did in lab6
```


## Question: How do I automate each shapiro test so that all the variables are in the same table?
For now, I am not going to do a shapiro test because it's unlikely a model with all the inputs I'm about to add is linear. Instead, I will do a correlation test to see if I need to add interaction terms.
Here is a list of the variables I will be selecting:

- aridity
- p_mean
- high_prec_freq
- high_prec_dur
- low_prec_freq
- lai_max
- water_frac
- slope_mean

### Now, I'm going to make a correlation matrix for these variables to see in interaction terms are necessary



```{r, echo = TRUE}
camels |> 
  select(q_mean, aridity, p_mean, high_prec_freq, high_prec_dur, low_prec_freq, lai_max, water_frac, slope_mean) |> 
  drop_na() |> 
  cor()
```


The following have strong correlations:

- q_mean and low_prec_freq have a strong negative correlation
- q_mean and p_mean have a strong positive correlation
- aridity and p_mean have strong negative correlations
- aridity and low_prec_freq have strong positive correlations
- aridity and lai_max have strong negative correlations
- p_mean and low_prec_freq have strong positive correlations
- high_prec_freq and low_prec_freq have strong positive correlations

... so for these pairs we will make interaction terms in the recipe.

for all of the variables that have strong correlations, there are many more that have weaker ones, so we will continue with nonparametric modeling.


### One last strategy to see if we can normalize the data. From the last column in skimr, I could see that none of the predictor variables are normally distributed. Now, let's test the logged version of each variable.



```{r, echo = TRUE}
camelog <- camels %>% 
  mutate(logarid = log(aridity),
         logp_mean = log(p_mean),
         logprecfreq = log(high_prec_freq),
         logprecdur = log(high_prec_dur),
         loglowprecfreq = log(low_prec_freq),
         loglaimax = log(lai_max),
         logwaterfrac = log(water_frac),
         logslopemean = log(slope_mean)) %>% 
  select(!c(aridity, p_mean, high_prec_freq, high_prec_dur, low_prec_freq, lai_max, water_frac, slope_mean, p_seasonality, frac_snow, high_prec_timing, low_prec_dur, geol_1st_class, glim_1st_class_frac, geol_2nd_class, glim_2nd_class_frac, carbonate_rocks_frac, geol_porostiy, geol_permeability, soil_depth_pelletier, soil_depth_statsgo, soil_porosity, soil_conductivity, max_water_content, sand_frac, silt_frac, clay_frac, organic_frac, other_frac, lai_diff, gvf_max, gvf_diff, dom_land_cover_frac, dom_land_cover, root_depth_50, root_depth_99, q_mean, runoff_ratio, slope_fdc, baseflow_index, stream_elas, q5, q95, high_q_freq, high_q_dur, low_q_dur, zero_q_freq, hfd_mean))

```



#### Checking normality in logged variables



```{r, echo = TRUE}
shapiro.test(camelog$logarid)

gghistogram(camelog$logarid)


```

```{r, echo = TRUE}
shapiro.test(camelog$logp_mean)

gghistogram(camelog$logp_mean)
```

```{r, echo = TRUE}
shapiro.test(camelog$logprecfreq)

gghistogram(camelog$logprecfreq)
```

```{r, echo = TRUE}
shapiro.test(camelog$logprecdur)

gghistogram(camelog$logprecdur)
```

```{r, echo = TRUE}
shapiro.test(camelog$loglowprecfreq)
gghistogram(camelog$loglowprecfreq)
```

```{r, echo = TRUE}
gghistogram(camelog$loglaimax)

gghistogram(camels$lai_max)

#this one doesn't really make a difference whether or not I log it; it isn't normal either way
```



All of this logging with no normal distributions further supports my decision to use models that don't assume normal distribution, especially when using so many predictor variables.

### Training and Testing Data


```{r, echo = TRUE}
set.seed(225)

camels_split3 <- initial_split(camels, prop = 0.8)
camels_train3 <- training(camels_split3)
camels_test3  <- testing(camels_split3)

camels_cv3 <- vfold_cv(camels_train3, v = 10)

```




## The recipe after checking all predictor variables

### predictors to try in the recipe
- aridity
- p_mean
- high_prec_freq
- high_prec_dur
- low_prec_freq
- lai_max
- water_frac
- slope_mean

### Interaction terms to add to the recipe
- q_mean:low_prec_freq
- q_mean:p_mean
- aridity:p_mean
- aridity:low_prec_freq
- aridity:lai_max
- p_mean:low_prec_freq
- high_prec_freq:low_prec_freq



```{r, echo = TRUE}
rec3 <-  recipe(q_mean ~ aridity + p_mean + high_prec_freq + low_prec_freq + high_prec_dur + lai_max + water_frac + slope_mean, data = camels_train3) %>%
  # Log transform the predictor variables (aridity and p_mean)
  #step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean + aridity:low_prec_freq +
                          aridity:lai_max + p_mean:low_prec_freq +
                           high_prec_freq:low_prec_freq) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())

```



The goal of the modelbuilding is still to predict q_mean, so how is this different from lab 6?

## Building Candidate Models


```{r, echo = TRUE}
#Random Forest Model
rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

#Neural Network Model
library(baguette)
library(xgboost)
nnet_model <- bag_mlp() %>% 
  set_engine("nnet") %>% 
  set_mode("regression")

#Xgboost Model
library(parsnip)
xgboost_model <- boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

#linear regression model
lm_model <- linear_reg() %>%
  # define the engine
  set_engine("lm") %>%
  # define the mode
  set_mode("regression")

```

```{r, echo = TRUE}
wf3 <- workflow_set(list(rec3), list(lm_model, rf_model, nnet_model, xgboost_model)) %>%
#map models to recipes
    workflow_map('fit_resamples', resamples = camels_cv3) 

autoplot(wf3)

rank_results(wf3, rank_metric = "rsq", select_best = TRUE)

```


## 4. Select a model you think best performs

At an r-squared of .922, the random forest model beats out the competition. Therefore, I select this model, especially because there's no requirement for explainability.

rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

The engine is ranger, which creates a large amount of decision trees, and the mode is regression, which predicts continuous values such as streamflow. This combination is performing well because having more decision trees, which can reduce variance and reduce the risk of overfitting, which is what happens when the model learns noisy data like streamflow too well.


## Building random forest model


```{r, echo = TRUE}
rf_wf3 <- workflow() %>%
  # Add the recipe
  add_recipe(rec3) %>%
  # Add the model
  add_model(rf_model) %>%
  # Fit the model to the training data
  fit(data = camels_train3)

```



## Hyperparameter Specifications

What are hyperparameters?
Hyperparameters are settings that control the learning process of a model.
They are set before training and affect the model’s performance.
Hyperparameters can be tuned to optimize the model’s predictive power.



```{r, echo = TRUE}
# I'm sticking with ranger and regression
rf_model_tune <- rand_forest(
  mtry = tune(),
  trees = tune(), 
  min_n = tune()
  #	Minimum # of data points in a node before it can be split
) %>%
  set_engine("ranger", importance = "impurity", seed = 123) %>%
  set_mode("regression")

```



Fitting the testing data to the tuned model



```{r, echo = TRUE}
# Set up tuning grid
rf_grid <- grid_regular(
  mtry(range = c(1, 3)),  # Adjust range based on your # of predictors
  trees(range = c(100, 500)),
  min_n(range = c(2, 20)),
  levels = 5  # Number of values per parameter
)

print(rf_grid)

```

```{r, echo = TRUE}
rf_wf_3_tune <- workflow() %>%
  # Add the recipe
  add_recipe(rec3) %>%
  # Add the model
  add_model(rf_model_tune) %>%
  # Fit the model to the training data
  fit(data = camels_train3)

```

```{r, echo = TRUE}
rf_data3 <- augment(rf_wf3_tune, new_data = camels_test3)
dim(rf_data3)
```

```{r, echo = TRUE}
metrics(rf_data3, truth = q_mean, estimate = .pred)
```

```{r, echo = TRUE}

```

```{r, echo = TRUE}

```

```{r, echo = TRUE}

```

```{r, echo = TRUE}

```

```{r, echo = TRUE}

```

```{r, echo = TRUE}

```

```{r, echo = TRUE}

```

```{r, echo = TRUE}

```

```{r, echo = TRUE}

```

```{r, echo = TRUE}

```

```{r, echo = TRUE}

```

```{r, echo = TRUE}

```

```{r, echo = TRUE}

```

```{r, echo = TRUE}

```

```{r, echo = TRUE}

```

```{r, echo = TRUE}

```


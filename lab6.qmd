---
title: 'Lab 6: Machine Learning in Hydrology'
output-dir: docs
format:
   html:
    code-fold: true
    toc: true
subtitle: 'Using Tidymodels & CAMELS Data'
---

tidymodels is an R framework designed for machine learning and statistical modeling. Built on the principles of the tidyverse, tidymodels provides a consistent and modular approach to tasks like data preprocessing, model training, evaluation, and validation. By leveraging the strengths of packages such as recipes, parsnip, and yardstick, tidymodels streamlines the modeling workflow, making it easier to experiment with different models while maintaining reproducibility and interpretability.

loading packages for analysis

```{r, echo = TRUE}
library(tidymodels)
library(ggplot2)
library(ggpubr)
library(visdat)
library(sf)
library(parsnip)
library(recipes)
library(yardstick)
library(tidyverse)


library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(ggthemes)
```

Why the hype about stream data? This is a large dataset that's helped to enhance deep learning and model tuning while being a tool for understanding model behavior. It facilitates large-scale model comparisons and allows for hybrid models that combine physics based models with machine learning.

What’s in the data? Each record in the CAMELS dataset represents a unique river basin, identified by an outlet USGS NWIS gauge_id. The dataset contains a mix of continuous and categorical variables, including meteorological, catchment, and streamflow summaries.

The data we are going to downloaded are the basin level summaries. For example, if we looked at row 1 of the data (Gage: 01013500) all of the values are the areal average for the drainage basin, while the flow metrics are associated with the outlet gage.

The CAMELS dataset is hosted by NCAR and can be accessed here under the “Individual Files” section. The root URL for all data seen on the “Individual Files” page is:

```{r, echo = TRUE}
root  <- 'https://gdex.ucar.edu/dataset/camels/file'
```

Near the bottom of that page, there are many .txt files that contain the data we want. Some hold climate data for each basin, some hold geology data, some hold soil data, etc. There is also a PDF with descriptions of the columns in each file. We are going to download all of the .txt files and the PDF.

## Getting the documentation PDF

```{r, echo = TRUE}
file <- 'https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf'

download.file(file, "C:/Users/Joshua Puyear/Documents/csu-undergrad/ess-330-joshp-2025/github/ess-330-labs/06-hydrology-ml/docs/camels_attributes.pdf", mode = "wb")

```

## Getting Basin Characteristics

The glue package provides an efficient way to interpolate and manipulate strings. It is particularly useful for dynamically constructing text, formatting outputs, and embedding R expressions within strings.

Now we want to download the .txt files that store the actual data documented in the PDF. Doing this file by file (like we did with the PDF) is possible, but lets look at a better/easier way…

#### a. Lets create a vector storing the data types/file names we want to download:

```{r, echo = TRUE}
types <- c("clim", "geol", "soil", "topo", "vege", "hydro")
```

#### b. Using glue, we can construct the needed URLs and file names for the data we want to download:

```{r, echo = TRUE}
# Where the files live online ...
remote_files  <- glue('{root}/camels_{types}.txt')
# where we want to download the data ...
local_files   <- glue('data/camels_{types}.txt')
```

#### c. Now we can download the data: walk2 comes from the purrr package and is used to apply a function to multiple arguments in parallel (much like map2 works over paired lists). Here, we are asking walk2 to pass the first element of remote_files and the first element of local_files to the download.file function to download the data, and setting quiet = TRUE to suppress output. The process is then iterated for the second element of each vector, and so on.

```{r, echo = TRUE}
walk2(remote_files, local_files, download.file, quiet = TRUE)

```

#### d. Once downloaded, the data can be read into R using readr::read_delim(), again instead of applying this to each file individually, we can use map to apply the function to each element of the local_files list.

```{r, echo = TRUE}
# Read and merge data
camels <- map(local_files, read_delim, show_col_types = FALSE) 
```

#### e. This gives us a list of data.frames, one for each file that we want to merge into a single table. So far in class we have focused on \*\_join functions to merge data based on a primary and foreign key relationship.

In this current list, we have \>2 tables, but, have a shared column called gauge_id that we can use to merge the data. However, since we have more then a left and right hand table, we need a more robust tool. We will use the powerjoin package to merge the data into a single data frame. powerjoin is a flexible package for joining lists of data.frames. It provides a wide range of join types, including inner, left, right, full, semi, anti, and cross joins making it a versatile tool for data manipulation and analysis, and one that should feel familiar to users of dplyr.

In this case, we are join to merge every data.frame in the list (n = 6) by the shared gauge_id column. Since we want to keep all data, we want a full join.

```{r, echo = TRUE}
camels <- power_full_join(camels ,by = 'gauge_id')
```

# Question 1: Your Turn (10 points)

## Exploratory Data Analysis

First, lets make a map of the sites. Use the borders() ggplot function to add state boundaries to the map and initially color the points by the mean flow (q_mean) at each site.

```{r, echo = TRUE}
ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = q_mean)) +
  scale_color_gradient(low = "pink", high = "dodgerblue") +
  ggthemes::theme_map()

```

#### Q1 Answers

At this point, all of the data and the PDF are downloaded into my directory

zero_q_freq represents frequency of days with Q = 0 mm/day, and is listed as a percentage.

# Question 2: Your Turn (10 points)

#### Make 2 maps of the sites, coloring the points by aridity and the p_mean collumn. Add clear labels, titles, and a color scale that makes sense for each parameter

```{r, echo = TRUE}
aridity_map <- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = aridity)) +
  scale_color_gradient(
    low = "skyblue", high = "red")+
  ggthemes::theme_map() +
  labs(
    title = "Aridity Across the US",
    subtitle = "Ratio of Mean Potential \nEvapotranspiration (Priestly-Taylor Index) to \nMean Precipitation",
    color = "Aridity"
    )

#How do I change the color scaling on here to get a more even split between dark blue and red values?
```

```{r, echo = TRUE}
precip_map <- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = p_mean)) +
  scale_color_gradient(low = "orange", high = "navy") +
  ggthemes::theme_map()+
  labs(
    title = "Mean Daily Precipitation \nacross the US",
    color = "Mean Daily \nPrecip (mm)")

#still not a huge fan of the scaling and would like to know how to fix this, but for now we'll go for it.

```

#### Ensure these render as a single image with your choice of facet_*, patchwork, or ggpubr

```{r, echo = TRUE}
library(patchwork)
aridity_map+precip_map
```



## Model Preparation

As an initial analysis, lets look at the relationship between aridity, rainfall and mean flow. First, lets make sure there is not significant correlation between these variables. Here, we make sure to drop NAs and only view the 3 columns of interest.

```{r, echo = TRUE}
#making a correlation matrix

camels |> 
  select(aridity, p_mean, q_mean) |> 
  drop_na() |> 
  cor()

```

Even though aridity has a strong inverse correlation with streamflow and prcip has a strong positive correlation with streamflow, we're still using these variables to predict the model

## Visual EDA

#### a. Lets start by looking that the 3 dimensions (variables) of this data. We’ll start with a XY plot of aridity and rainfall. We are going to use the scale_color_viridis_c() function to color the points by the q_mean column. This scale functions maps the color of the points to the values in the q_mean column along the viridis continuous (c) palette. Because a scale_color\_\* function is applied, it maps to the known color aesthetic in the plot.

```{r, echo = TRUE}
# Create a scatter plot of aridity vs rainfall
ggplot(camels, aes(x = aridity, y = p_mean)) +
  # Add points colored by mean flow
  geom_point(aes(color = q_mean)) +
  # Add a linear regression line
  geom_smooth(method = "lm", color = "red", linetype = 2) +
  # Apply the viridis color scale
  scale_color_viridis_c() +
  # Add a title, axis labels, and theme (w/ legend on the bottom)
  theme_linedraw() + 
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")

```

By showing an x-y axis and color, it is possible for two variables to predict a third. These three dimensions can explain what's happening with streamflow. At this point, we're just showing what's happening in the available data.

So it looks like there is a relationship between rainfall, aridity, and rainfall but it looks like an exponential decay function and is certainly not linear.

To test a transformation, we can log transform the x and y axes using the scale_x_log10() and scale_y_log10() functions:

```{r, echo = TRUE}
#when you plot the logged version of this, the relationship becomes linear because of how logs turn multiplicative properties into additive ones.

ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  scale_color_viridis_c() +
  # Apply log transformations to the x and y axes
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom") + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow")
```

Great! We can see a log-log relationship between aridity and rainfall provides a more linear relationship. This is a common relationship in hydrology and is often used to estimate rainfall in ungauged basins. However, once the data are transformed, the lack of spread in the streamflow data is quite evident with high mean flow values being compressed to the low end of aridity/high end of rainfall.

To address this, we can visualize how a log transform may benifit the q_mean data as well. Since the data is represented by color, rather then an axis, we can use the trans (transform) argument in the scale_color_viridis_c() function to log transform the color scale.

```{r, echo = TRUE}
ggplot(camels, aes(x = aridity, y = p_mean)) +
  geom_point(aes(color = q_mean)) +
  geom_smooth(method = "lm") +
  # Apply a log transformation to the color scale
  scale_color_viridis_c(trans = "log") +
  scale_x_log10() + 
  scale_y_log10() +
  theme_linedraw() +
  theme(legend.position = "bottom",
        # Expand the legend width ...
        legend.key.width = unit(2.5, "cm"),
        legend.key.height = unit(.5, "cm")) + 
  labs(title = "Aridity vs Rainfall vs Runnoff", 
       x = "Aridity", 
       y = "Rainfall",
       color = "Mean Flow") 

```

Excellent! Treating these three right skewed variables as log transformed, we can see a more evenly spread relationship between aridity, rainfall, and mean flow. This is a good sign for building a model to predict mean flow using aridity and rainfall.

# Model Building
##splitting the data

First, we set a seed for reproducabilty, then transform the q_mean column to a log scale. Remember it is error prone to apply transformations to the outcome variable within a recipe. So, we’ll do it a prioi.

Once set, we can split the data into a training and testing set. We are going to use 80% of the data for training and 20% for testing with no stratification.

Additionally, we are going to create a 10-fold cross validation dataset to help us evaluate multi-model setups.

```{r, echo = TRUE}
set.seed(123)
# Bad form to perform simple transformations on the outcome variable within a 
# recipe. So, we'll do it here.
camels <- camels |> 
  mutate(logQmean = log(q_mean))

# Generate the split
camels_split <- initial_split(camels, prop = 0.8)
camels_train <- training(camels_split)
camels_test  <- testing(camels_split)

camels_cv <- vfold_cv(camels_train, v = 10)
```

## Preprocessor: recipe
In lecture, we have focused on using formulas as a workflow preprocessor. Separately we have used the recipe function to define a series of data preprocessing steps. Here, we are going to use the recipe function to define a series of data preprocessing steps.

We learned quite a lot about the data in the visual EDA. We know that the q_mean, aridity and p_mean columns are right skewed and can be helped by log transformations. We also know that the relationship between aridity and p_mean is non-linear and can be helped by adding an interaction term to the model. To implement these, lets build a recipe!

```{r, echo = TRUE}
# Create a recipe to preprocess the data
rec <-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %>%
  # Log transform the predictor variables (aridity and p_mean)
  step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())
```

## Naive base lm approach
Ok, to start, lets do what we are comfortable with … fitting a linear model to the data. First, we use prep and bake on the training data to apply the recipe. Then, we fit a linear model to the data.

```{r, echo = TRUE}
# Prepare the data
baked_data <- prep(rec, camels_train) |> 
  bake(new_data = NULL)

# Interaction with lm
#  Base lm sets interaction terms with the * symbol
lm_base <- lm(logQmean ~ aridity * p_mean, data = baked_data)
summary(lm_base)

```

```{r, echo = TRUE}
# Sanity Interaction term from recipe ... these should be equal!!
summary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))

```
## Where things get a little messy…

Ok so now we have our trained model lm_base and want to validate it on the test data.

Remember a models ability to predict on new data is the most important part of the modeling process. It really doesnt matter how well it does on data it has already seen!

We have to be careful about how we do this with the base R approach:

*Don't use augment directly on the test data before preprocessing has been applied to the data, otherwise the values it will add to the data will be incorrect
*Don't use predict on the test data before preprocessing steps have been applied to the test data

## Correct version: prep -> bake -> predict
To correctly evaluate the model on the test data, we need to apply the same preprocessing steps to the test data that we applied to the training data. We can do this using the prep and bake functions with the recipe object. This ensures the test data is transformed in the same way as the training data before making predictions.

```{r, echo = TRUE}
test_data <-  bake(prep(rec), new_data = camels_test)
test_data$lm_pred <- predict(lm_base, newdata = test_data)
```

## Model Evaluation: statistical and visual

```{r, echo = TRUE}
metrics(test_data, truth = logQmean, estimate = lm_pred)
```
```{r, echo = TRUE}
ggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +
  # Apply a gradient color scale
  scale_color_gradient2(low = "brown", mid = "orange", high = "darkgreen") +
  geom_point() +
  geom_abline(linetype = 2) +
  theme_linedraw() + 
  labs(title = "Linear Model: Observed vs Predicted",
       x = "Observed Log Mean Flow",
       y = "Predicted Log Mean Flow",
       color = "Aridity")

```

## Using a workflow instead

tidymodels provides a framework for building and evaluating models using a consistent and modular workflow. The workflows package allows you to define a series of modeling steps, including data preprocessing, model fitting, and model fitting, in a single object. This makes it easier to experiment with different models, compare performance, and ensure reproducibility.

workflows are built from a model, a preprocessor, and a execution. Here, we are going to use the linear_reg function to define a linear regression model, set the engine to lm, and the mode to regression. We then add our recipe to the workflow, fit the model to the training data, and extract the model coefficients.

```{r, echo = TRUE}
# Define model
lm_model <- linear_reg() %>%
  # define the engine
  set_engine("lm") %>%
  # define the mode
  set_mode("regression")

# Instantiate a workflow ...
lm_wf <- workflow() %>%
  # Add the recipe
  add_recipe(rec) %>%
  # Add the model
  add_model(lm_model) %>%
  # Fit the model to the training data
  fit(data = camels_train) d

# Extract the model coefficients from the workflow
summary(extract_fit_engine(lm_wf))$coefficients

```
Lets ensure we replicated the results from the lm_base model. How do they look to you?

```{r, echo = TRUE}
# From the base implementation
summary(lm_base)$coefficients

```
## Making Predictions

Now that lm_wf is a workflow, data is not embedded in the model, we can use augment with the new_data argument to make predictions on the test data.

```{r, echo = TRUE}
#
lm_data <- augment(lm_wf, new_data = camels_test)
dim(lm_data)

```
## Model Evaluation: Statistical and Visual
As with EDA, applying for graphical and statistical evaluation of the model is a key Here, we use the metrics function to extract the default metrics (rmse, rsq, mae) between the observed and predicted mean streamflow values.

We then create a scatter plot of the observed vs predicted values, colored by aridity, to visualize the model performance.

```{r, echo = TRUE}
metrics(lm_data, truth = logQmean, estimate = .pred)
```

```{r, echo = TRUE}
ggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +
  scale_color_viridis_c() +
  geom_point() +
  geom_abline() +
  theme_linedraw()

```

## Switch it up!
The real power of this approach is that we can easily switch out the models/recipes and see how it performs. Here, we are going to instead use a random forest model to predict mean streamflow. We define a random forest model using the rand_forest function, set the engine to ranger, and the mode to regression. We then add the recipe, fit the model, and evaluate the skill.


```{r, echo = TRUE}


```

```{r, echo = TRUE}


```


```{r, echo = TRUE}


```


```{r, echo = TRUE}


```

```{r, echo = TRUE}


```

```{r, echo = TRUE}


```

```{r, echo = TRUE}


```



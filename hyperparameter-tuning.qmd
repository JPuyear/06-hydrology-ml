---
title: "Machine Learning Pipline for Regression"
subtitle: "CAMELS hydrological data"
date: "2025-04-18"
author: 
  name: Josh Puyear
  email: "puyearjosh@gmail.com" 
project:
 output-dir: docs
format: html
execute:
  echo: true
---

```{r, echo = TRUE}
library(tidyverse)
library(tidymodels)
```

libraries for data reading
```{r, echo = TRUE}
library(tidyverse)
library(tidymodels)
library(powerjoin)
library(glue)
library(vip)
library(baguette)
library(visdat)
library(ggpubr)
library(skimr)
library(broom)

```

### Warning: You can no longer download the data from the website
Data download
```{r, echo = TRUE}
#root  <- 'https://gdex.ucar.edu/dataset/camels/file'
```

PDF of Data Documentation
```{r, echo = TRUE}
#file <- 'https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf'
#download.file(file, "C:/Users/Joshua Puyear/Documents/csu-undergrad/ess-330-joshp-2025/github/ess-330-labs/06-hydrology-ml/docs/camels_attributes.pdf", mode = "wb")
```

Basin Characteristics
```{r, echo = TRUE}
#types <- c("clim", "geol", "soil", "topo", "vege", "hydro")

```

Constructing URLS with Glue
```{r, echo = TRUE}
# Where the files live online ...
#remote_files  <- glue('{root}/camels_{types}.txt')
# where we want to download the data ...
#local_files   <- glue('data/camels_{types}.txt')

```

Important: Downloading the Data with purrr walk2
```{r, echo = TRUE}
#walk2(remote_files, local_files, download.file, quiet = TRUE)
```

Finally: Mapping the data to the local_files list with map and read_delim
```{r, echo = TRUE}
# Read and merge data
#camels <- map(local_files, read_delim, show_col_types = FALSE) 
```

Performing a power_full_join to merge all tables by guage_id column
```{r, echo = TRUE}
#camels <- power_full_join(camels, by = 'gauge_id')
```

```{r, echo = TRUE}
camels <- read_csv("C:/Users/Joshua Puyear/Documents/csu-undergrad/ess-330-joshp-2025/github/ess-330-labs/06-hydrology-ml/data/camels.csv")
```

Exploratory Data Analysis: EDA Steps

## 1. Question: What are the main predictors of mean streamflow in the Poudre basin?
## 2.Data have already been read in

## 3. Summarizing the Data

### Basic Attributes with Skim
```{r, echo = TRUE}
skim(camels)
#in the last column, skim gives you a general histogram shape

```
52/58 columns are numeric and there are 671 total observations, well within the range for visdat. Skim is useful if we're concered about the size of a dataset

### Visualization
```{r, echo = TRUE}
vis_dat(camels)
```
Most of the missing values come from geol_2nd_class and root_depth_99, which we don't really care about for streamflow. Looks like a pretty complete dataset! 

### Data Summary

```{r, echo = TRUE}
summary(camels)
```

Since the median mean streamflow (q_mean) is lower than the mean streamflow, there are probably spikes of much higher streamflow interspersed with longer periods of low flow. The range of mean streamflow is between .004 mm/day (basically nothing) and 9.68mm/day. Mean precipitation p_mean is close to the median, at 3.25 and 3.22, respectively. The minimum is .6446 while the maximum is 8.94. These ranges all seem reasonable.

# cleaning the data with dplyr, etc
```{r, echo = TRUE}
library(patchwork)

gghistogram(log(camels$q_mean))

gghistogram(log(camels$p_mean))

#would log transformation normalize log p mean, which has a long tail?
#I will use step_log for any predictors that need to be logged. This will be tested with the shapiro test, which looks for normal distribution

#logging doesn't make the normality much better, and the models I'm going to choose will not be linear anyway and thus will function without normal distribution

#How can a table of shapiro test results be made?
shapiro.test(camels$q_mean)
#very much not normal data

#to see how the variables relate, find the cor test you did in lab6
```
## Question: How do I automate each shapiro test so that all the variables are in the same table?
For now, I am not going to do a shapiro test because it's unlikely a model with all the inputs I'm about to add is linear. Instead, I will do a correlation test to see if I need to add interaction terms.
Here is a list of the variables I will be selecting:

- aridity
- p_mean
- high_prec_freq
- high_prec_dur
- low_prec_freq
- lai_max
- water_frac
- slope_mean

### Now, I'm going to make a correlation matrix for these variables to see in interaction terms are necessary

```{r, echo = TRUE}
camels |> 
  select(q_mean, aridity, p_mean, high_prec_freq, high_prec_dur, low_prec_freq, lai_max, water_frac, slope_mean) |> 
  drop_na() |> 
  cor()
```
The following have strong correlations:

- q_mean and low_prec_freq have a strong negative correlation
- q_mean and p_mean have a strong positive correlation
- aridity and p_mean have strong negative correlations
- aridity and low_prec_freq have strong positive correlations
- aridity and lai_max have strong negative correlations
- p_mean and low_prec_freq have strong positive correlations
- high_prec_freq and low_prec_freq have strong positive correlations

... so for these pairs we will make interaction terms in the recipe.

for all of the variables that have strong correlations, there are many more that have weaker ones, so we will continue with nonparametric modeling.


### One last strategy to see if we can normalize the data. From the last column in skimr, I could see that none of the predictor variables are normally distributed. Now, let's test the logged version of each variable.

```{r, echo = TRUE}
camelog <- camels %>% 
  mutate(logarid = log(aridity),
         logp_mean = log(p_mean),
         logprecfreq = log(high_prec_freq),
         logprecdur = log(high_prec_dur),
         loglowprecfreq = log(low_prec_freq),
         loglaimax = log(lai_max),
         logwaterfrac = log(water_frac),
         logslopemean = log(slope_mean)) %>% 
  select(!c(aridity, p_mean, high_prec_freq, high_prec_dur, low_prec_freq, lai_max, water_frac, slope_mean, p_seasonality, frac_snow, high_prec_timing, low_prec_dur, geol_1st_class, glim_1st_class_frac, geol_2nd_class, glim_2nd_class_frac, carbonate_rocks_frac, geol_porostiy, geol_permeability, soil_depth_pelletier, soil_depth_statsgo, soil_porosity, soil_conductivity, max_water_content, sand_frac, silt_frac, clay_frac, organic_frac, other_frac, lai_diff, gvf_max, gvf_diff, dom_land_cover_frac, dom_land_cover, root_depth_50, root_depth_99, q_mean, runoff_ratio, slope_fdc, baseflow_index, stream_elas, q5, q95, high_q_freq, high_q_dur, low_q_dur, zero_q_freq, hfd_mean))

```

#### Checking normality in logged variables

```{r, echo = TRUE}
shapiro.test(camelog$logarid)

gghistogram(camelog$logarid)
```


All of this logging with no normal distributions further supports my decision to use models that don't assume normal distribution, especially when using so many predictor variables.

### Training and Testing Data
```{r, echo = TRUE}
set.seed(225)

camels_split3 <- initial_split(camels, prop = 0.8)
camels_train3 <- training(camels_split3)
camels_test3  <- testing(camels_split3)

camels_cv3 <- vfold_cv(camels_train3, v = 10)

```


## The recipe after checking all predictor variables

### predictors to try in the recipe
- aridity
- p_mean
- high_prec_freq
- high_prec_dur
- low_prec_freq
- lai_max
- water_frac
- slope_mean

### Interaction terms to add to the recipe
- q_mean:low_prec_freq
- q_mean:p_mean
- aridity:p_mean
- aridity:low_prec_freq
- aridity:lai_max
- p_mean:low_prec_freq
- high_prec_freq:low_prec_freq

```{r, echo = TRUE}
rec3 <-  recipe(q_mean ~ aridity + p_mean + high_prec_freq + low_prec_freq + high_prec_dur + lai_max + water_frac + slope_mean, data = camels_train3) %>%
  # Log transform the predictor variables (aridity and p_mean)
  #step_log(all_predictors()) %>%
  # Add an interaction term between aridity and p_mean
  step_interact(terms = ~ aridity:p_mean + aridity:low_prec_freq +
                          aridity:lai_max + p_mean:low_prec_freq +
                           high_prec_freq:low_prec_freq) |> 
  # Drop any rows with missing values in the pred
  step_naomit(all_predictors(), all_outcomes())

```

The goal of the modelbuilding is still to predict q_mean, so how is this different from lab 6?

## Building Candidate Models
```{r, echo = TRUE}
#Random Forest Model
rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

#Neural Network Model
library(baguette)
library(xgboost)
nnet_model <- bag_mlp() %>% 
  set_engine("nnet") %>% 
  set_mode("regression")

#Xgboost Model
library(parsnip)
xgboost_model <- boost_tree() %>% 
  set_engine("xgboost") %>% 
  set_mode("regression")

#linear regression model
lm_model <- linear_reg() %>%
  # define the engine
  set_engine("lm") %>%
  # define the mode
  set_mode("regression")

```

```{r, echo = TRUE}
wf3 <- workflow_set(list(rec3), list(lm_model, rf_model, nnet_model, xgboost_model)) %>%
#map models to recipes
    workflow_map('fit_resamples', resamples = camels_cv3) 

autoplot(wf3)

rank_results(wf3, rank_metric = "rsq", select_best = TRUE)

```
## 4. Select a model you think best performs

At an r-squared of .922, the random forest model beats out the competition. Therefore, I select this model, especially because there's no requirement for explainability.

rf_model <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

The engine is ranger, which creates a large amount of decision trees, and the mode is regression, which predicts continuous values such as streamflow. This combination is performing well because having more decision trees, which can reduce variance and reduce the risk of overfitting, which is what happens when the model learns noisy data like streamflow too well.

## Chosen Model with Hyperparameter Specifications
What are hyperparameters?
Hyperparameters are settings that control the learning process of a model.
They are set before training and affect the modelâ€™s performance.
Hyperparameters can be tuned to optimize the modelâ€™s predictive power.

```{r, echo = TRUE}
# I'm sticking with ranger and regression

rf_model_tune <- rand_forest(
  mtry = tune(),
  trees = tune(), 
  min_n = tune()
) %>%
  set_engine("ranger", importance = "impurity", seed = 123) %>%
  set_mode("regression")

```

## Create a workflow from chosen model
```{r, echo = TRUE}
rf_wf3_tune <- workflow() %>%
  add_recipe(rec3) %>%
  add_model(rf_model_tune)

```

## Check Tunable Values/Ranges

```{r, echo = TRUE}
dials_params <- extract_parameter_set_dials(rf_wf3_tune) %>% 
  update(
    mtry = mtry(range = c(1, 10)),
    trees = trees(range = c(200, 800)),
    min_n = min_n(range = c(5, 15))
  ) %>%
  finalize(x = camels_train3 %>% select(-q_mean))

# Verify parameters
dials_params$object
```
```{r, echo = TRUE}
print(dials_params)
```

```{r, echo = TRUE}
set.seed(123)

my.grid <- grid_latin_hypercube(dials_params,
  size = 20
)

print(my.grid)
```

## Tuning the Model

```{r, echo = TRUE, results = 'hide'}
model_params <- tune_grid(
  rf_wf3_tune,          # Your tuned workflow
  resamples = camels_cv3,  # Your cross-validation folds (previously created)
  grid = my.grid,       # Your parameter grid
  metrics = metric_set(rmse, rsq, mae),  # Evaluation metrics
  control = control_grid(
    save_pred = TRUE,   # Save predictions for analysis
    verbose = TRUE      # Show progress
  )
)

```

## Autoplotting

```{r, echo = TRUE}
autoplot_results <- autoplot(model_params) +
  labs(title = "Random Forest Hyperparameter Tuning Results",
       subtitle = "Performance across parameter combinations") +
  theme_minimal()

print(autoplot_results)

```
Mean average error does not show a clear relationship with randomly selected predictors, number of trees, or minimal node size. Root mean squared error increases with randomly selected predictors, but there is no effect of rmse of more trees or of an increase in minimal node size. R-squared decreases as randomly selected predictors increase, but the effect on rsq is not clear with increasing tree size or node size. Main takeaway is the fewer randomly selected trees, the better, but the other parameters don't matter as much. 
- As the number of randomly selected predictors increases, r squared decreasees and the error increases. Thus, it's better to have fewer randomly selected predictors for accuracy. Mean average error stays about the same except for an outlier at the very lowest amount of predictors.
- As the number of trees increases, there is no change in rmse or rsq. At the lowest and highest ends, it looks like the number of trees is random. Mean average error doesn't change with this number of trees either.
- Minimal node size stays has no effect on mae, appears to have a weak negative correlation with rmse, and no correlation with rsq.


### Collecting Metrics
```{r, echo = TRUE}
model_params %>% 
  collect_metrics() %>% 
  filter(.metric == "rsq") %>% 
  slice_max(mean, n = 5)

```

Showing the Best Outcome
```{r, echo = TRUE}
# Show best performing combinations
show_best(model_params, metric = "rmse", n = 5)
show_best(model_params, metric = "rsq", n = 5)  
```
Just like the autoplot showed, a lower mtry has a higher rsq and lower rmse. The min_n is pretty random, as is the number of trees, but the top value of mtry = 3, trees = 592, and min_n = 14 is the same for metric = rmse and metric = rsq.

### Best Parameters
```{r, echo = TRUE}

hp_best <- select_best(model_params)

```

### Finalizing Workflow
```{r, echo = TRUE}
final_wf <- finalize_workflow(rf_wf3_tune, hp_best)

```

### Fitting Finalized Workflow to Data Split
```{r, echo = TRUE}

lastft <- last_fit(final_wf, camels_split3)
```
Checking Performance of last model
```{r, echo = TRUE}

collect_metrics(lastft)

```
The performance is .03 higher in rsq and .04 lower in rmse, so better overall than the training data alone. At an rsq of .95, this means 95 percent of the variability can be explained by the model. According to the RMSE, predictions are off by .38 units on average, whcih could be bad if the data aren't fractional, since a high rmse makes models less accurate.


### Collecting Predictions 

```{r, echo = TRUE}
predictions <- collect_predictions(lastft)

```


### Interpret these results. How does the final model perform on the test data? Is it better or worse than the training data? Use your knowledge of the regression based metrics to describe the results.

This is actually really good  because we have a lower rmse than with the testing data alone and the rsq is .959, better than when we collected metrics on the tuned model. .959 r-squared means 95.9 percent of the variability is explained by the model. It captures almost all of the variability influencing streamflow. there might be some overfitting, which could be verified by giving the model more data.  

## Ggplotting Predicted vs actual values
```{r, echo = TRUE}

predictions %>% 
ggplot(aes(x = .pred, y = q_mean)) +
          geom_smooth(method = "lm")+
          geom_abline() +
          labs(title = "Actual vs Predicted Values for A Random Forest Streamflow Model",
               subtitle = "CAMELS Dataset",
               x = "Predicted",
               y = "Actual")

```

```{r, echo = TRUE}
finalft <- fit(final_wf, camels)

final_augment <- augment(finalft, camels) %>% 
  select(.pred, q_mean, gauge_lon, gauge_lat) %>% 
  mutate(res = (.pred - q_mean)^2)

```

## Mapping Our Results
```{r, echo = TRUE}
library(ggplot2)
library(patchwork)

# Custom theme to center titles
centered_theme <- function() {
  ggthemes::theme_map() +
    theme(
      plot.title = element_text(hjust = 0.5, size = 12, face = "bold")  # 0.5 = center
    )
}

# Predicted Streamflow Map
predmap <- ggplot(data = final_augment, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = .pred)) +
  scale_color_gradient(low = "pink", high = "dodgerblue", name = "Predicted Flow") +
  centered_theme() +  # Use the custom theme
  labs(title = "Predicted Streamflow")

# Residuals Map
resid <- ggplot(data = final_augment, aes(x = gauge_lon, y = gauge_lat)) +
  borders("state", colour = "gray50") +
  geom_point(aes(color = res)) +
  scale_color_gradient2(
    low = "skyblue", mid = "white", high = "maroon", 
    midpoint = 0, name = "Residuals"  # Diverging scale for residuals
  ) +
  centered_theme() +
  labs(title = "Residuals")

# Combine plots
predmap / resid +
  plot_annotation(title = "Model Performance Across Watersheds", 
                 theme = theme(plot.title = element_text(hjust = 0.5, size = 14)))
```


